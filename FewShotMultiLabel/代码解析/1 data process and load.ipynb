{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理模块\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**目录：**\n",
    "\n",
    "0. 数据读取\n",
    "\n",
    "1. 句子特征提取\n",
    "\n",
    "2. 标签类别收集\n",
    "\n",
    "3. 样本转化为模型可读的特征\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aTsg5j62xfnk"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "from ordered_set import OrderedSet\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, RandomSampler, DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 训练数据形式回顾\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 两个数据集\n",
    "\n",
    "<img src=\"./数据结构0.PNG\"  width=\"400\" height=\"600\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集格式\n",
    "\n",
    "Stanford数据集有三个domain，所以一个domain做train，一个做dev，一个做test；\n",
    "\n",
    "json文件的key就是domain名称；\n",
    "\n",
    "数据文件中有一层不必要的list，导致代码会报错；需要注意改过来；\n",
    "\n",
    "<img src=\"./数据结构1.PNG\"  width=\"300\" height=\"300\" align=\"left\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个episode的数据\n",
    "\n",
    "1. 每个episode中\"support\"的句子数量是不一定的；\n",
    "\n",
    "2. \"seq_outs\"是指slot标签数据，这里是没有提供的，也是本文不会去研究的；\n",
    "\n",
    "<img src=\"./数据结构2.PNG\"  width=\"300\" height=\"300\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回忆 support set需要是minimum including的，\n",
    "\n",
    "1. domain下每个标签都要有至少K次 (K-SHOT)；\n",
    "2. 去掉一个样本，上述条件就不满足；\n",
    "\n",
    "<img src=\"./数据结构3.PNG\"  width=\"300\" height=\"300\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 数据集加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "import json\n",
    "import collections\n",
    "import logging\n",
    "\n",
    "import sys\n",
    "from typing import List, Tuple, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawDataLoaderBase:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def load_data(self, path: str):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User(name='tester', sex='男', age='12')\n",
      "tester\n",
      "男\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "# 定义一个输入样本的格式 （注意，这不是一个 小样本学习 的样本）\n",
    "\n",
    "DataItem = collections.namedtuple(\"DataItem\", [\"seq_in\", \"seq_out\", \"label\"])\n",
    "\n",
    "# namedtuple\n",
    "# 因为元组的局限性：不能为元组内部的数据进行命名，\n",
    "#     所以往往我们并不知道一个元组所要表达的意义，\n",
    "#     所以在这里引入了 collections.namedtuple 这个工厂函数，\n",
    "#     来构造一个带字段名的元组\n",
    "\n",
    "import collections\n",
    "\n",
    "# 两种方法来给 namedtuple 定义方法名\n",
    "User = collections.namedtuple('User', ['name', 'sex', 'age'])\n",
    "# User = collections.namedtuple('User', 'name age id')\n",
    "user = User('tester', '男', '12')\n",
    "\n",
    "print(user)\n",
    "\n",
    "# 获取用户的属性\n",
    "print(user.name)\n",
    "print(user.sex)\n",
    "print(user.age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FewShotExample(object):\n",
    "    \"\"\"  \n",
    "    \n",
    "    Each few-shot example is a pair of (one query example, support set) \n",
    "    \n",
    "    每个小样本学习 的样本，是query example，配上一个support set；\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            gid: int,\n",
    "            batch_id: int,\n",
    "            test_id: int,\n",
    "            domain_name: str,\n",
    "            support_data_items: List[DataItem],\n",
    "            test_data_item: DataItem\n",
    "    ):\n",
    "        self.gid = gid\n",
    "        self.batch_id = batch_id\n",
    "        self.test_id = test_id  # query relative index in one episode\n",
    "        self.domain_name = domain_name\n",
    "\n",
    "        self.support_data_items = support_data_items  # all support data items\n",
    "        self.test_data_item = test_data_item  # one query data items\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'gid:{}\\n\\tdomain:{}\\n\\ttest_data:{}\\n\\ttest_label:{}\\n\\tsupport_data:{}'.format(\n",
    "            self.gid,\n",
    "            self.domain_name,\n",
    "            self.test_data_item.seq_in,\n",
    "            self.test_data_item.seq_out,\n",
    "            self.support_data_items,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FewShotRawDataLoader(RawDataLoaderBase):\n",
    "    def __init__(self, opt):\n",
    "        super(FewShotRawDataLoader, self).__init__()\n",
    "        self.opt = opt\n",
    "        self.debugging = opt.do_debug\n",
    "\n",
    "    def load_data(self, path: str) -> (List[FewShotExample], List[List[FewShotExample]], int):\n",
    "        \"\"\"\n",
    "            load few shot data set\n",
    "            input:\n",
    "                path: file path\n",
    "            output\n",
    "                examples: a list, all example loaded from path\n",
    "                few_shot_batches: a list, of fewshot batch, each batch is a list of examples\n",
    "                max_len: max sentence length\n",
    "            \"\"\"\n",
    "        with open(path, 'r') as reader:\n",
    "            raw_data = json.load(reader)\n",
    "            examples, few_shot_batches, max_support_size = \\\n",
    "                self.raw_data2examples(raw_data)\n",
    "        if self.debugging:\n",
    "            examples, few_shot_batches = examples[:8], few_shot_batches[:2]\n",
    "        return examples, few_shot_batches, max_support_size\n",
    "\n",
    "    def raw_data2examples(self, raw_data: Dict) -> (List[FewShotExample], List[List[FewShotExample]], int):\n",
    "        \"\"\"\n",
    "        process raw_data into examples\n",
    "        \"\"\"\n",
    "        examples = []\n",
    "        all_support_size = []\n",
    "        few_shot_batches = []\n",
    "        for domain_n, domain in raw_data.items():\n",
    "            \n",
    "            # Notice: the batch here means few shot batch, not training batch\n",
    "            for batch_id, batch in enumerate(domain[0]):\n",
    "\n",
    "                one_batch_examples = []\n",
    "                support_data_items, test_data_items = self.batch2data_items(batch) \n",
    "                # \"support\"; \n",
    "                # \"query\":\n",
    "                \n",
    "                all_support_size.append(len(support_data_items))\n",
    "                ''' Pair each test sample with full support set '''\n",
    "                for test_id, test_data_item in enumerate(test_data_items):\n",
    "                    gid = len(examples)\n",
    "                    example = FewShotExample(\n",
    "                        gid=gid,\n",
    "                        batch_id=batch_id,\n",
    "                        test_id=test_id,\n",
    "                        domain_name=domain_n,\n",
    "                        test_data_item=test_data_item,\n",
    "                        support_data_items=support_data_items,\n",
    "                    )\n",
    "                    examples.append(example)\n",
    "                    one_batch_examples.append(example)\n",
    "                few_shot_batches.append(one_batch_examples)\n",
    "\n",
    "        # print(\"all_support_size: \", all_support_size)\n",
    "        max_support_size = max(all_support_size)\n",
    "        return examples, few_shot_batches, max_support_size\n",
    "\n",
    "    def batch2data_items(self, batch: dict) -> (List[DataItem], List[DataItem]):\n",
    "\n",
    "        support_data_items = self.get_data_items(parts=batch['support'])\n",
    "        test_data_items = self.get_data_items(parts=batch['query'])\n",
    "\n",
    "        return support_data_items, test_data_items\n",
    "\n",
    "    def get_data_items(self, parts: dict) -> List[DataItem]:\n",
    "        # 将一个个数据转化为 DataItem，也就是一个namedTuple\n",
    "\n",
    "        data_item_lst = []\n",
    "        for seq_in, seq_out, label in zip(parts['seq_ins'], parts['seq_outs'], parts['labels']):\n",
    "            # todo: move word-piecing into preprocessing module\n",
    "            # label = token_label if self.opt.task == 'ml' else sent_label   # decide label type according to task\n",
    "            data_item = DataItem(seq_in=seq_in, seq_out=seq_out, label=label)\n",
    "            # print(data_item)\n",
    "            data_item_lst.append(data_item)\n",
    "        return data_item_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "num of examples:  6400\n",
      "--------------------------------------------------\n",
      "the first example:  gid:0\n",
      "\tdomain:weather\n",
      "\ttest_data:['Thank', 'you', 'very', 'much', 'car', '!']\n",
      "\ttest_label:['O', 'O', 'O', 'O', 'O', 'O']\n",
      "\tsupport_data:[DataItem(seq_in=['I', 'need', 'to', 'know', 'what', 'the', 'lowest', 'temperature', 'will', 'be', 'in', 'New', 'York', 'in', 'the', 'next', 'few', 'days', '.'], seq_out=['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], label=['request_low_temperature']), DataItem(seq_in=['Which', 'one', 'is', 'gon', 'na', 'be', 'the', 'lowest', 'temperature', 'today', 'and', 'tomorrow', 'in', 'Fresno', '?'], seq_out=['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], label=['request_time']), DataItem(seq_in=['thank', 'you'], seq_out=['O', 'O'], label=['appreciate']), DataItem(seq_in=['Find', 'what', 'the', 'temperature', 'will', 'be', 'in', 'downtown', 'Chicago', 'this', 'weekend', '.'], seq_out=['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], label=['request_temperature']), DataItem(seq_in=['what', 'is', 'the', 'weather', 'for', 'the', 'next', '7', 'days'], seq_out=['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], label=['request_weather']), DataItem(seq_in=['Exeter'], seq_out=['O'], label=['inform']), DataItem(seq_in=['What', 'is', 'the', 'current', 'high', 'temperature', 'in', 'Atherton', '?'], seq_out=['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], label=['request_high_temperature']), DataItem(seq_in=['Currently', 'are', 'there', 'clear', 'skies', 'in', 'redwood', 'city', '?'], seq_out=['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], label=['query'])]\n",
      "--------------------------------------------------\n",
      "num of batches:  200\n",
      "--------------------------------------------------\n",
      "the first batch:  32\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "### 数据加载： 举例\n",
    "\n",
    "class Config():\n",
    "    do_debug = False\n",
    "\n",
    "opt = Config()\n",
    "opt.do_debug = False\n",
    "\n",
    "data_loader = FewShotRawDataLoader(opt)\n",
    "\n",
    "path = \"../data/stanford/stanford.0.spt_s_1.q_s_32.ep_200--use_schema--label_num_schema2/train.json\"\n",
    "examples, few_shot_batches, max_support_size = data_loader.load_data(path)\n",
    "print(\"-\" * 50)\n",
    "print(\"num of examples: \", len(examples))\n",
    "print(\"-\" * 50)\n",
    "print(\"the first example: \", examples[0])\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"num of batches: \", len(few_shot_batches))\n",
    "print(\"-\" * 50)\n",
    "print(\"the first batch: \", len(few_shot_batches[0]))\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label集合收集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "def flatten(l):\n",
    "    \"\"\" convert list of list to list\"\"\"\n",
    "    return [item  for sublist in l for item in sublist]\n",
    "    # 不能写成 [item for item in sublist for sublist in l ]\n",
    "\n",
    "l_0 = [[4, 5], [2, 3]]\n",
    "print(flatten(l_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "\n",
    "def make_dict(opt, examples: List[FewShotExample]) -> (Dict[str, int], Dict[int, str]):\n",
    "    \"\"\"\n",
    "    make label2id dict\n",
    "    label2id must follow rules:\n",
    "    For sequence labeling:\n",
    "        1. id(PAD)=0 id(O)=1  2. id(B-X)=i  id(I-X)=i+1\n",
    "    For (multi-label) text classification:\n",
    "        1. id(PAD)=0\n",
    "    \"\"\"\n",
    "    \n",
    "    # 对槽位标签的\n",
    "    def purify(l):\n",
    "        \"\"\" remove B- and I- \"\"\"\n",
    "        return set([item.replace('B-', '').replace('I-', '') for item in l])\n",
    "\n",
    "    ''' collect all label from: all test set & all support set '''\n",
    "    all_labels = []\n",
    "    label2id = {}\n",
    "    for example in examples:\n",
    "        if opt.task == 'sl':  # 在这里不适用\n",
    "            all_labels.append(example.test_data_item.seq_out)\n",
    "            all_labels.extend([data_item.seq_out for data_item in example.support_data_items])\n",
    "        else:\n",
    "            all_labels.append(example.test_data_item.label)\n",
    "            all_labels.extend([data_item.label for data_item in example.support_data_items])\n",
    "    ''' collect label word set '''\n",
    "    label_set = sorted(list(purify(set(flatten(all_labels)))))  # sort to make embedding id fixed\n",
    "    \n",
    "#     # transfer label to index type such as `label_1`\n",
    "#     # 这里我们是跳过的，不需要\n",
    "#     if opt.index_label:\n",
    "#         if 'label2index_type' not in opt:\n",
    "#             opt.label2index_type = {}\n",
    "#             for idx, label in enumerate(label_set):\n",
    "#                 opt.label2index_type[label] = 'label_' + str(idx)\n",
    "#         else:\n",
    "#             max_label_idx = max([int(value.replace('label_', '')) for value in opt.label2index_type.values()])\n",
    "#             for label in label_set:\n",
    "#                 if label not in opt.label2index_type:\n",
    "#                     max_label_idx += 1\n",
    "#                     opt.label2index_type[label] = 'label_' + str(max_label_idx)\n",
    "#         label_set = [opt.label2index_type[label] for label in label_set]\n",
    "#     elif opt.unused_label:\n",
    "#         if 'label2unused_type' not in opt:\n",
    "#             opt.label2unused_type = {}\n",
    "#             for idx, label in enumerate(label_set):\n",
    "#                 opt.label2unused_type[label] = '[unused' + str(idx) + ']'\n",
    "#         else:\n",
    "#             max_label_idx = max([int(value.replace('[unused', '').replace(']', '')) for value in opt.label2unused_type.values()])\n",
    "#             for label in label_set:\n",
    "#                 if label not in opt.label2unused_type:\n",
    "#                     max_label_idx += 1\n",
    "#                     opt.label2unused_type[label] = '[unused' + str(max_label_idx) + ']'\n",
    "#         label_set = [opt.label2unused_type[label] for label in label_set]\n",
    "#     else:\n",
    "#         pass\n",
    "    \n",
    "    ''' build dict '''\n",
    "    label2id['[PAD]'] = len(label2id)  # '[PAD]' in first position and id is 0\n",
    "    if opt.task == 'sl':\n",
    "        label2id['O'] = len(label2id)\n",
    "        for label in label_set:\n",
    "            if label == 'O':\n",
    "                continue\n",
    "            label2id['B-' + label] = len(label2id)\n",
    "            label2id['I-' + label] = len(label2id)\n",
    "    else:  # mlc. sc\n",
    "        for label in label_set:\n",
    "            label2id[label] = len(label2id)\n",
    "    ''' reverse the label2id '''\n",
    "    id2label = dict([(idx, label) for label, idx in label2id.items()])\n",
    "    return label2id, id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[PAD]': 0, 'appreciate': 1, 'inform': 2, 'query': 3, 'request_high_temperature': 4, 'request_low_temperature': 5, 'request_temperature': 6, 'request_time': 7, 'request_weather': 8}\n"
     ]
    }
   ],
   "source": [
    "# 举例：\n",
    "\n",
    "opt.task = \"stanford\"\n",
    "opt.index_label = False\n",
    "opt.unused_label = False\n",
    "\n",
    "label2id, id2label = make_dict(opt, examples)\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 句子特征提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用stanford pos-tagger做词性标注\n",
    "\n",
    "https://nlp.stanford.edu/software/tagger.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"./stanford-nlp-group-photo.jpg\"  width=\"900\" height=\"900\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pos_tagger.PNG\"  width=\"900\" height=\"900\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from nltk.tag import StanfordPOSTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tagger(model_dir):\n",
    "    tagger_model_file = os.path.join(model_dir, 'models/english-bidirectional-distsim.tagger')\n",
    "    tagger_jar_file = os.path.join(model_dir, 'stanford-postagger.jar')\n",
    "    tagger = StanfordPOSTagger(model_filename=tagger_model_file, path_to_jar=tagger_jar_file)\n",
    "    return tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag_data(tagger, prefix=''):\n",
    "    all_sub_dirs = os.listdir(DATA_DIR)\n",
    "    all_sub_dirs = [sub_dir for sub_dir in all_sub_dirs if sub_dir.startswith(prefix)]\n",
    "    print('all_sub_dirs: {}'.format(all_sub_dirs))\n",
    "    json_file_lst = ['train.json', 'dev.json', 'test.json']\n",
    "\n",
    "    res = {}\n",
    "    d_count = 0\n",
    "\n",
    "    if DEBUG:\n",
    "        all_sub_dirs = all_sub_dirs[:1]\n",
    "        json_file_lst = ['dev.json']\n",
    "\n",
    "    for sub_dir in all_sub_dirs:\n",
    "        print(sub_dir)\n",
    "        for json_file in json_file_lst:\n",
    "            print(json_file)\n",
    "            filename = os.path.join(DATA_DIR, sub_dir, json_file)\n",
    "            with open(filename, 'r') as fr:\n",
    "                json_data = json.load(fr)\n",
    "            for key, episode_data in json_data.items():\n",
    "                print(key)\n",
    "                print(type(episode_data))\n",
    "                print(len(episode_data))\n",
    "                print(len(episode_data[0]))\n",
    "                for e_item in episode_data[0]:\n",
    "                    # print(e_item)\n",
    "                    # print(e_item['support'])\n",
    "                    for seq_in in e_item['support']['seq_ins']:\n",
    "                        text = ' '.join(seq_in)\n",
    "                        if text not in res:\n",
    "                            d_count += 1\n",
    "                            tag_data = tagger.tag(seq_in)\n",
    "                            # print(seq_in)\n",
    "                            # print(tag_data)\n",
    "                            res[text] = tag_data\n",
    "                            if d_count % 10 == 0:\n",
    "                                print('d_count - {}'.format(d_count))\n",
    "\n",
    "                    for seq_in in e_item['query']['seq_ins']:\n",
    "                        text = ' '.join(seq_in)\n",
    "                        if text not in res:\n",
    "                            d_count += 1\n",
    "                            tag_data = tagger.tag(seq_in)\n",
    "                            res[text] = tag_data\n",
    "                            if d_count % 10 == 0:\n",
    "                                print('d_count - {}'.format(d_count))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 举例\n",
    "\n",
    "\n",
    "['So', 'this', 'is', 'more', 'high', '-tech', \"it's\", 'what', 'I', 'think', 'people', 'like', 'today', '.']\n",
    "\n",
    "[('So', 'RB'), ('this', 'DT'), ('is', 'VBZ'), ('more', 'RBR'), ('high', 'JJ'), ('-tech', 'IN'), (\"it's\", 'PRP$'), ('what', 'WP'), ('I', 'PRP'), ('think', 'VBP'), ('people', 'NNS'), ('like', 'IN'), ('today', 'NN'), ('.', '.')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预处理器\n",
    "\n",
    "分为 InputBuilder, OutputBuilder, FeatureConstructor三个组成；\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### InputBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureItem = collections.namedtuple(   # text or raw features\n",
    "    \"FeatureItem\",\n",
    "    [\n",
    "        \"tokens\",  # tokens corresponding to input token ids, eg: word_piece tokens with [CLS], [SEP]\n",
    "        \"labels\",  # labels for all input position, eg; label for word_piece tokens\n",
    "        \"data_item\",\n",
    "        \"token_ids\",\n",
    "        \"segment_ids\",\n",
    "        \"nwp_index\",\n",
    "        \"input_mask\",\n",
    "        \"output_mask\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "ModelInput = collections.namedtuple(  # digit features for computation\n",
    "    \"ModelInput\",   # all element shape: test: (1, test_len) support: (support_size, support_len)\n",
    "    [\n",
    "        \"token_ids\",  # token index list\n",
    "        \"segment_ids\",  # bert [SEP] ids\n",
    "        \"nwp_index\",  # non-word-piece word index to extract non-word-piece tokens' reps (only useful for bert).\n",
    "        \"input_mask\",  # [1] * len(sent), 1 for valid (tokens, cls, sep, word piece), 0 is padding in batch construction\n",
    "        \"output_mask\",  # [1] * len(sent), 1 for valid output, 0 for padding, eg: 1 for original tokens in sl task\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# InputBuilder的基础父类\n",
    "\n",
    "class InputBuilderBase:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, example, max_support_size, label2id\n",
    "    ) -> (FeatureItem, ModelInput, List[FeatureItem], ModelInput):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def data_item2feature_item(self, data_item: DataItem, seg_id: int) -> FeatureItem:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_test_model_input(self, feature_item: FeatureItem) -> ModelInput:\n",
    "        # FeatureItem 转化为 torch tensor, 这样就可以作为模型的输入\n",
    "        ret = ModelInput(\n",
    "            token_ids=torch.LongTensor(feature_item.token_ids),\n",
    "            segment_ids=torch.LongTensor(feature_item.segment_ids),\n",
    "            nwp_index=torch.LongTensor(feature_item.nwp_index),\n",
    "            input_mask=torch.LongTensor(feature_item.input_mask),\n",
    "            output_mask=torch.LongTensor(feature_item.output_mask)\n",
    "        )\n",
    "        return ret\n",
    "\n",
    "    def get_support_model_input(self, feature_items: List[FeatureItem], max_support_size: int) -> ModelInput:\n",
    "        pad_id = self.tokenizer.vocab['[PAD]']\n",
    "        token_ids = self.pad_support_set([f.token_ids for f in feature_items], pad_id, max_support_size)\n",
    "        segment_ids = self.pad_support_set([f.segment_ids for f in feature_items], 0, max_support_size)\n",
    "        nwp_index = self.pad_support_set([f.nwp_index for f in feature_items], [0], max_support_size)\n",
    "        input_mask = self.pad_support_set([f.input_mask for f in feature_items], 0, max_support_size)\n",
    "        output_mask = self.pad_support_set([f.output_mask for f in feature_items], 0, max_support_size)\n",
    "        ret = ModelInput(\n",
    "            token_ids=torch.LongTensor(token_ids),\n",
    "            segment_ids=torch.LongTensor(segment_ids),\n",
    "            nwp_index=torch.LongTensor(nwp_index),\n",
    "            input_mask=torch.LongTensor(input_mask),\n",
    "            output_mask=torch.LongTensor(output_mask)\n",
    "        )\n",
    "        return ret\n",
    "\n",
    "    def pad_support_set(self, item_lst: List[List[int]], pad_value: int, max_support_size: int) -> List[List[int]]:\n",
    "        \"\"\"\n",
    "        pre-pad support set to insure: \n",
    "            1. each spt set has same sent num \n",
    "            2. each sent has same length\n",
    "        (do padding here because: \n",
    "                1. all support sent are considered as one tensor input  \n",
    "                2. support set size is small\n",
    "            )\n",
    "        :param item_lst:\n",
    "        :param pad_value:\n",
    "        :param max_support_size:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        ''' pad sentences '''\n",
    "        max_sent_len = max([len(x) for x in item_lst])  # max length among one\n",
    "        ret = []\n",
    "        for sent in item_lst:\n",
    "            temp = sent[:]\n",
    "            while len(temp) < max_sent_len:\n",
    "                temp.append(pad_value)\n",
    "            ret.append(temp)\n",
    "        \n",
    "        ''' pad support set size '''\n",
    "        pad_item = [pad_value for _ in range(max_sent_len)]\n",
    "        while len(ret) < max_support_size:\n",
    "            ret.append(pad_item)\n",
    "        return ret\n",
    "\n",
    "    def digitizing_input(self, tokens: List[str], seg_id: int) -> (List[int], List[int]):\n",
    "        # tokens 映射为 ids\n",
    "        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        segment_ids = [seg_id for _ in range(len(tokens))]\n",
    "        return token_ids, segment_ids\n",
    "\n",
    "    def tokenizing(self, item: DataItem):\n",
    "        \"\"\"\n",
    "        Possible tokenizing for item \n",
    "        不同的模型，可能不一样\n",
    "        \n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelNumSchemaInputBuilder\n",
    "\n",
    "# 先看 BertInputBuilder\n",
    "class BertInputBuilder(InputBuilderBase):\n",
    "    def __init__(self, tokenizer, opt):\n",
    "        super(BertInputBuilder, self).__init__(tokenizer)\n",
    "        self.opt = opt\n",
    "        self.test_seg_id = 0\n",
    "        self.support_seg_id = 0 if opt.context_emb == 'sep_bert' else 1  \n",
    "        # 1 to cat support and query to get reps \n",
    "        # 0 表示support 和 query 分别计算BERT表征；\n",
    "        \n",
    "        self.tag_data_dict = None\n",
    "        if opt.task == 'mlc':\n",
    "            # 把所有被词性标注过的句子加载进内存\n",
    "            self.tag_data_dict = self.load_tag_data_dict()\n",
    "        \n",
    "        self.seq_ins = {}\n",
    "\n",
    "    def __call__(self, example, max_support_size, label2id) -> (FeatureItem, ModelInput, List[FeatureItem], ModelInput):\n",
    "        # query转化为特征\n",
    "        test_feature_item, test_input = self.prepare_test(example)\n",
    "        \n",
    "        # support句子转化为特征\n",
    "        support_feature_items, support_input = self.prepare_support(example, max_support_size)\n",
    "        \n",
    "        return test_feature_item, test_input, support_feature_items, support_input\n",
    "\n",
    "    def prepare_test(self, example):\n",
    "        test_feature_item = self.data_item2feature_item(data_item=example.test_data_item, seg_id=0)\n",
    "        test_input = self.get_test_model_input(test_feature_item)\n",
    "        return test_feature_item, test_input\n",
    "\n",
    "    def prepare_support(self, example, max_support_size):\n",
    "        support_feature_items = [self.data_item2feature_item(data_item=s_item, seg_id=self.support_seg_id) for s_item in\n",
    "                                 example.support_data_items]\n",
    "        support_input = self.get_support_model_input(support_feature_items, max_support_size)\n",
    "        return support_feature_items, support_input\n",
    "\n",
    "    def data_item2feature_item(self, data_item: DataItem, seg_id: int) -> FeatureItem:\n",
    "        \"\"\" \n",
    "        get feature_item for bert, steps: \n",
    "            1. do digitalizing \n",
    "            2. make mask \n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        # 进行subword分词\n",
    "        wp_mark, wp_text = self.tokenizing(data_item)\n",
    "        \n",
    "        if self.opt.task == 'sl':  # use word-level labels  [opt.label_wp is supported by model now.]\n",
    "            labels = self.get_wp_label(data_item.seq_out, wp_text, wp_mark) if self.opt.label_wp else data_item.seq_out\n",
    "        \n",
    "        else:  # use sentence level labels\n",
    "            labels = data_item.label\n",
    "#             if 'None' not in labels:\n",
    "#                 # transfer label to index type such as `label_1`\n",
    "#                 # 没有用到\n",
    "#                 if self.opt.index_label:\n",
    "#                     labels = [self.opt.label2index_type[label] for label in labels]\n",
    "#                 if self.opt.unused_label:\n",
    "#                     labels = [self.opt.label2unused_type[label] for label in labels]\n",
    "        \n",
    "        # 拼接成BERT的输入格式\n",
    "        tokens = ['[CLS]'] + wp_text + ['[SEP]'] if seg_id == 0 else wp_text + ['[SEP]']\n",
    "        # 转化为id\n",
    "        token_ids, segment_ids = self.digitizing_input(tokens=tokens, seg_id=seg_id)\n",
    "        \n",
    "        \n",
    "        nwp_index = self.get_nwp_index(wp_mark)\n",
    "        input_mask = [1] * len(token_ids)\n",
    "        output_mask = [1] * len(labels)   # For sl: it is original tokens; For mlc: it is labels\n",
    "        \n",
    "        ret = FeatureItem(\n",
    "            tokens=tokens,\n",
    "            labels=labels,\n",
    "            data_item=data_item,\n",
    "            token_ids=token_ids,\n",
    "            segment_ids=segment_ids,\n",
    "            nwp_index=nwp_index,\n",
    "            input_mask=input_mask,\n",
    "            output_mask=output_mask,\n",
    "        )\n",
    "        return ret\n",
    "\n",
    "    def get_nwp_index(self, word_piece_mark: list) -> torch.Tensor:\n",
    "        \"\"\" get index of non-word-piece tokens, which is used to extract non-wp bert embedding in batch manner \"\"\"\n",
    "        # 没有被wordpiece拆开的单词地方标为1；        \n",
    "        return torch.nonzero(torch.LongTensor(word_piece_mark) - 1).tolist()  # wp mark word-piece with 1, so - 1\n",
    "\n",
    "    def tokenizing(self, item: DataItem):\n",
    "        \"\"\" Do tokenizing and get word piece data and get label on pieced words. \"\"\"\n",
    "        wp_text = self.tokenizer.wordpiece_tokenizer.tokenize(' '.join(item.seq_in))\n",
    "        \n",
    "        # wp_mark： 如果单词被拆分为多个wordpiece tokens，单词的后面几个wp token位置标位1\n",
    "        wp_mark = [int((len(w) > 2) and w[0] == '#' and w[1] == '#') for w in wp_text]  # mark wp as 1\n",
    "        return wp_mark, wp_text\n",
    "\n",
    "    def get_wp_label(self, label_lst, wp_text, wp_mark, label_pieced_words=False):\n",
    "        \"\"\" get label on pieced words. \"\"\"\n",
    "        # 本文用不到\n",
    "        wp_label, label_idx = [], 0\n",
    "        for ind, mark in enumerate(wp_mark):\n",
    "            if mark == 0:  # label non-pieced token with original label\n",
    "                wp_label.append(label_lst[label_idx])\n",
    "                label_idx += 1  # pointer on non-wp labels\n",
    "            elif mark == 1:  # label word-piece with whole word's label or with  [PAD] label\n",
    "                pieced_label = wp_label[-1].replace('B-', 'I-') if label_pieced_words else '[PAD]'\n",
    "                wp_label.append(pieced_label)\n",
    "            if not wp_label[-1]:\n",
    "                raise RuntimeError('Empty label')\n",
    "        if not (len(wp_label) == len(wp_text) == len(wp_mark)):\n",
    "            raise RuntimeError('ERROR: Failed to generate wp labels:{}{}{}{}{}{}{}{}{}{}{}'.format(\n",
    "                len(wp_label), len(wp_text), len(wp_mark),\n",
    "                '\\nwp_lb', wp_label, '\\nwp_text', wp_text, '\\nwp_mk', wp_mark, '\\nlabel', label_lst))\n",
    "\n",
    "    def prepare_label_num_features(self, example, label2id):\n",
    "        # support的label number features\n",
    "        support_label_num_features = [self.extract_label_num_feature(s_item) for s_item in example.support_data_items]\n",
    "        \n",
    "        # torch.stack：是concat操作，但是是会自己先添加一个dimension进行拼接，\n",
    "        support_label_num_features = torch.stack(support_label_num_features, dim=0)\n",
    "        \n",
    "        # query的label number features\n",
    "        test_label_num_features = self.extract_label_num_feature(example.test_data_item)\n",
    "        return support_label_num_features, test_label_num_features\n",
    "\n",
    "    def prepare_label_num_target(self, example, label2id):\n",
    "        # label数量\n",
    "        support_label_num_target = torch.Tensor([len(s_item.label) for s_item in example.support_data_items])\n",
    "        test_label_num_target = torch.Tensor([len(example.test_data_item.label)])\n",
    "        return support_label_num_target, test_label_num_target\n",
    "\n",
    "    def extract_label_num_feature(self, item: DataItem) -> torch.Tensor:\n",
    "        seq_in = item.seq_in\n",
    "        seq_in_text = ' '.join(seq_in)\n",
    "        if seq_in_text in self.seq_ins:\n",
    "            return self.seq_ins[seq_in_text]\n",
    "        else:\n",
    "            sent_len = len(seq_in)\n",
    "            # tag_data = self.tagger.tag(seq_in)\n",
    "            if seq_in_text not in self.tag_data_dict:\n",
    "                raise ValueError('the tag data dict is not complement ')  # complete\n",
    "            tag_data = self.tag_data_dict[seq_in_text]\n",
    "            conj_num = verb_num = punc_num = qst_num = 0\n",
    "            \n",
    "            # 计数： \n",
    "            for origin_item, tag_item in tag_data:\n",
    "                # 谓词\n",
    "                if tag_item in [\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]:\n",
    "                    verb_num += 1\n",
    "                # Coordinating conjunction: 并列连词\n",
    "                elif tag_item == \"CC\":\n",
    "                    conj_num += 1\n",
    "                \n",
    "                # 标点符号\n",
    "                elif tag_item in [\",\", \".\"] or origin_item == \";\":  # `! ?` are represent as `.`\n",
    "                    punc_num += 1\n",
    "                \n",
    "                # 疑问代词 & 疑问副词等\n",
    "                # WP: wh-pronoun; WRB: wh-adverb; WP$:  Possessive wh-pronoun; WDT wh-determiner\n",
    "                elif tag_item in [\"WP\", \"WRB\", \"WDZ\", \"WP$\"]:\n",
    "                    qst_num += 1\n",
    "            sf_item = torch.Tensor([sent_len, conj_num, verb_num, punc_num, qst_num])\n",
    "            self.seq_ins[seq_in_text] = sf_item\n",
    "        return sf_item\n",
    "\n",
    "    def load_tag_data_dict(self):\n",
    "        # 把所有被词性标注过的句子加载进内存\n",
    "        \n",
    "        tag_data_dir = os.path.dirname(os.path.dirname(self.opt.train_path))\n",
    "        tag_data_path = os.path.join(tag_data_dir, 'tag_data.dict.all')\n",
    "        with open(tag_data_path, 'r') as fr:\n",
    "            tag_data_dict = json.load(fr)\n",
    "        return tag_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label name的处理；\n",
    "\n",
    "class SchemaInputBuilder(BertInputBuilder):\n",
    "    def __init__(self, tokenizer, opt):\n",
    "        super(SchemaInputBuilder, self).__init__(tokenizer, opt)\n",
    "\n",
    "    def __call__(self, example, max_support_size, label2id) -> (FeatureItem, ModelInput, List[FeatureItem], ModelInput):\n",
    "        test_feature_item, test_input = self.prepare_test(example)\n",
    "        support_feature_items, support_input = self.prepare_support(example, max_support_size)\n",
    "        \n",
    "        # label名称的表征：主要用\"sep\"， BERT直接编码；不与句子拼接\n",
    "        if self.opt.label_reps in ['cat']:  # represent labels by concat all all labels\n",
    "            label_input, label_items = self.prepare_label_feature(label2id)\n",
    "        elif self.opt.label_reps in ['sep', 'sep_sum']:  # represent each label independently\n",
    "            label_input, label_items = self.prepare_sep_label_feature(label2id)\n",
    "        return test_feature_item, test_input, support_feature_items, support_input, label_items, label_input,\n",
    "\n",
    "    def prepare_label_feature(self, label2id: dict):\n",
    "        \"\"\" prepare digital input for label feature in concatenate style \"\"\"\n",
    "        \n",
    "        text, wp_text, label, wp_label, wp_mark = [], [], [], [], []\n",
    "        sorted_labels = sorted(label2id.items(), key=lambda x: x[1])\n",
    "        for label_name, label_id in sorted_labels:\n",
    "            if label_name == '[PAD]':\n",
    "                continue\n",
    "            tmp_text = self.convert_label_name(label_name)\n",
    "            tmp_wp_text = self.tokenizer.tokenize(' '.join(tmp_text))\n",
    "            text.extend(tmp_text)\n",
    "            wp_text.extend(tmp_wp_text)\n",
    "            label.extend(['O'] * len(tmp_text))\n",
    "            wp_label.extend(['O'] * len(tmp_wp_text))\n",
    "            wp_mark.extend([0] + [1] * (len(tmp_wp_text) - 1))\n",
    "        label_item = self.data_item2feature_item(DataItem(text, label, wp_text, wp_label, wp_mark), 0)\n",
    "        label_input = self.get_test_model_input(label_item)\n",
    "        return label_input, label_item\n",
    "\n",
    "    def prepare_sep_label_feature(self, label2id):\n",
    "        \"\"\" prepare digital input for label feature separately \"\"\"\n",
    "        label_items = []\n",
    "        for label_name in label2id:\n",
    "            if label_name == '[PAD]':\n",
    "                continue\n",
    "            seq_in = self.convert_label_name(label_name)\n",
    "            seq_out = ['None'] * len(seq_in)\n",
    "            label = ['None']\n",
    "            label_items.append(self.data_item2feature_item(DataItem(seq_in, seq_out, label), 0))\n",
    "        label_input = self.get_support_model_input(label_items, len(label2id) - 1)  # no pad, so - 1\n",
    "        return label_input, label_items\n",
    "\n",
    "    def convert_label_name(self, name):\n",
    "        text = []\n",
    "        tmp_name = name\n",
    "        if 'B-' in name:\n",
    "            text.append('begin')\n",
    "            tmp_name = name.replace('B-', '')\n",
    "        elif 'I-' in name:\n",
    "            text.append('inner')\n",
    "            tmp_name = name.replace('I-', '')\n",
    "        elif 'O' == name:\n",
    "            text.append('ordinary')\n",
    "            tmp_name = ''\n",
    "\n",
    "        # special processing to label name\n",
    "        name_translations = [('PER', 'person'), \n",
    "                             ('ORG', 'organization'), \n",
    "                             ('LOC', 'location'),\n",
    "                             ('MISC', 'miscellaneous'), \n",
    "                             ('GPE', 'geographical political'),\n",
    "                             ('NORP', 'nationalities or religious or political groups'),\n",
    "                             # toursg data\n",
    "                             (\"ACK\", \"acknowledgment, as well as common expressions used for grounding\"),\n",
    "                             # (\"CANCEL\", \"cancelation\"),\n",
    "                             # (\"CLOSING\", \"closing remarks\"),\n",
    "                             # (\"COMMIT\", \"commitment\"),\n",
    "                             # (\"CONFIRM\", \"confirmation\"),\n",
    "                             # (\"ENOUGH\", \"no more information is needed\"),\n",
    "                             # (\"EXPLAIN\", \"an explanation/justification of a previous stated idea\"),\n",
    "                             # (\"HOW_MUCH\", \"money or time amounts\"),\n",
    "                             # (\"HOW_TO\", \"used to request/give specific instructions\"),\n",
    "                             (\"INFO\", \"information request\"),\n",
    "                             # (\"NEGATIVE\", \"negative responses\"),\n",
    "                             # (\"OPENING\", \"opening remarks\"),\n",
    "                             # (\"POSITIVE\", \"positive responses\"),\n",
    "                             # (\"PREFERENCE\", \"preferences\"),\n",
    "                             # (\"RECOMMEND\", \"recommendations\"),\n",
    "                             # (\"THANK\", \"thank you remarks\"),\n",
    "                             # (\"WHAT\", \"concept related utterances\"),\n",
    "                             # (\"WHEN\", \"time related utterances\"),\n",
    "                             # (\"WHERE\", \"location related utterances\"),\n",
    "                             # (\"WHICH\", \"entity related utterances\"),\n",
    "                             # (\"WHO\", \"person related utterances and questions\"),\n",
    "                             ]\n",
    "        if tmp_name:\n",
    "            for shot, long in name_translations:\n",
    "                if tmp_name == shot:\n",
    "                    text.append(long)\n",
    "                    tmp_name = ''\n",
    "                    break\n",
    "        if tmp_name:\n",
    "            text.extend(tmp_name.lower().split('_'))\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelNumSchemaInputBuilder(SchemaInputBuilder):\n",
    "\n",
    "    def __init__(self, tokenizer, opt):\n",
    "        super(LabelNumSchemaInputBuilder, self).__init__(tokenizer, opt)\n",
    "\n",
    "    def __call__(self, example, max_support_size, label2id) -> (FeatureItem, ModelInput, List[FeatureItem], ModelInput):\n",
    "        test_feature_item, test_input = self.prepare_test(example)\n",
    "        support_feature_items, support_input = self.prepare_support(example, max_support_size)\n",
    "        \n",
    "        # 处理label名称\n",
    "        label_input = label_items = None\n",
    "        if self.opt.label_reps in ['cat']:  # represent labels by concat all all labels\n",
    "            label_input, label_items = self.prepare_label_feature(label2id)\n",
    "        elif self.opt.label_reps in ['sep', 'sep_sum']:  # represent each label independently\n",
    "            label_input, label_items = self.prepare_sep_label_feature(label2id)\n",
    "        \n",
    "        # label数量的特征\n",
    "        '''get sentence features'''\n",
    "        support_label_num_features, test_label_num_features = self.prepare_label_num_features(example, label2id)\n",
    "        '''get sentence target'''\n",
    "        support_label_num_target, test_label_num_target = self.prepare_label_num_target(example, label2id)\n",
    "\n",
    "        return test_feature_item, test_input, support_feature_items, support_input, label_items, label_input, \\\n",
    "            support_label_num_features, test_label_num_features, support_label_num_target, test_label_num_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 举例： \n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "opt.bert_vocab = \"../resources/bert_base_uncased/vocab.txt\"\n",
    "opt.context_emb = \"sep\"\n",
    "opt.label_reps = \"sep\"\n",
    "opt.task = \"mlc\"\n",
    "opt.train_path = \"../data/stanford/stanford.0.spt_s_1.q_s_32.ep_200--use_schema--label_num_schema2/train.json\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(opt.bert_vocab)\n",
    "input_builder = LabelNumSchemaInputBuilder(tokenizer, opt)\n",
    "\n",
    "\n",
    "test_feature_item, test_input, support_feature_items, support_input, label_items, label_input, \\\n",
    "            support_label_num_features, test_label_num_features, support_label_num_target, test_label_num_target = input_builder(\n",
    "    examples[0], max_support_size, label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_feature_item:  FeatureItem(tokens=['[CLS]', '[UNK]', 'you', 'very', 'much', 'car', '!', '[SEP]'], labels=['appreciate'], data_item=DataItem(seq_in=['Thank', 'you', 'very', 'much', 'car', '!'], seq_out=['O', 'O', 'O', 'O', 'O', 'O'], label=['appreciate']), token_ids=[101, 100, 2017, 2200, 2172, 2482, 999, 102], segment_ids=[0, 0, 0, 0, 0, 0, 0, 0], nwp_index=[[0], [1], [2], [3], [4], [5]], input_mask=[1, 1, 1, 1, 1, 1, 1, 1], output_mask=[1])\n",
      "test_input:  ModelInput(token_ids=tensor([ 101,  100, 2017, 2200, 2172, 2482,  999,  102]), segment_ids=tensor([0, 0, 0, 0, 0, 0, 0, 0]), nwp_index=tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5]]), input_mask=tensor([1, 1, 1, 1, 1, 1, 1, 1]), output_mask=tensor([1]))\n",
      "label_items:  [FeatureItem(tokens=['[CLS]', 'appreciate', '[SEP]'], labels=['None'], data_item=DataItem(seq_in=['appreciate'], seq_out=['None'], label=['None']), token_ids=[101, 9120, 102], segment_ids=[0, 0, 0], nwp_index=[[0]], input_mask=[1, 1, 1], output_mask=[1]), FeatureItem(tokens=['[CLS]', 'inform', '[SEP]'], labels=['None'], data_item=DataItem(seq_in=['inform'], seq_out=['None'], label=['None']), token_ids=[101, 12367, 102], segment_ids=[0, 0, 0], nwp_index=[[0]], input_mask=[1, 1, 1], output_mask=[1]), FeatureItem(tokens=['[CLS]', 'query', '[SEP]'], labels=['None'], data_item=DataItem(seq_in=['query'], seq_out=['None'], label=['None']), token_ids=[101, 23032, 102], segment_ids=[0, 0, 0], nwp_index=[[0]], input_mask=[1, 1, 1], output_mask=[1]), FeatureItem(tokens=['[CLS]', 'request', 'high', 'temperature', '[SEP]'], labels=['None'], data_item=DataItem(seq_in=['request', 'high', 'temperature'], seq_out=['None', 'None', 'None'], label=['None']), token_ids=[101, 5227, 2152, 4860, 102], segment_ids=[0, 0, 0, 0, 0], nwp_index=[[0], [1], [2]], input_mask=[1, 1, 1, 1, 1], output_mask=[1]), FeatureItem(tokens=['[CLS]', 'request', 'low', 'temperature', '[SEP]'], labels=['None'], data_item=DataItem(seq_in=['request', 'low', 'temperature'], seq_out=['None', 'None', 'None'], label=['None']), token_ids=[101, 5227, 2659, 4860, 102], segment_ids=[0, 0, 0, 0, 0], nwp_index=[[0], [1], [2]], input_mask=[1, 1, 1, 1, 1], output_mask=[1]), FeatureItem(tokens=['[CLS]', 'request', 'temperature', '[SEP]'], labels=['None'], data_item=DataItem(seq_in=['request', 'temperature'], seq_out=['None', 'None'], label=['None']), token_ids=[101, 5227, 4860, 102], segment_ids=[0, 0, 0, 0], nwp_index=[[0], [1]], input_mask=[1, 1, 1, 1], output_mask=[1]), FeatureItem(tokens=['[CLS]', 'request', 'time', '[SEP]'], labels=['None'], data_item=DataItem(seq_in=['request', 'time'], seq_out=['None', 'None'], label=['None']), token_ids=[101, 5227, 2051, 102], segment_ids=[0, 0, 0, 0], nwp_index=[[0], [1]], input_mask=[1, 1, 1, 1], output_mask=[1]), FeatureItem(tokens=['[CLS]', 'request', 'weather', '[SEP]'], labels=['None'], data_item=DataItem(seq_in=['request', 'weather'], seq_out=['None', 'None'], label=['None']), token_ids=[101, 5227, 4633, 102], segment_ids=[0, 0, 0, 0], nwp_index=[[0], [1]], input_mask=[1, 1, 1, 1], output_mask=[1])]\n",
      "label_input:  ModelInput(token_ids=tensor([[  101,  9120,   102,     0,     0],\n",
      "        [  101, 12367,   102,     0,     0],\n",
      "        [  101, 23032,   102,     0,     0],\n",
      "        [  101,  5227,  2152,  4860,   102],\n",
      "        [  101,  5227,  2659,  4860,   102],\n",
      "        [  101,  5227,  4860,   102,     0],\n",
      "        [  101,  5227,  2051,   102,     0],\n",
      "        [  101,  5227,  4633,   102,     0]]), segment_ids=tensor([[0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0]]), nwp_index=tensor([[[0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [0],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [1],\n",
      "         [2]],\n",
      "\n",
      "        [[0],\n",
      "         [1],\n",
      "         [2]],\n",
      "\n",
      "        [[0],\n",
      "         [1],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [1],\n",
      "         [0]],\n",
      "\n",
      "        [[0],\n",
      "         [1],\n",
      "         [0]]]), input_mask=tensor([[1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 0]]), output_mask=tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1]]))\n",
      "support_label_num_features:  tensor([[19.,  0.,  3.,  1.,  1.],\n",
      "        [15.,  1.,  3.,  1.,  0.],\n",
      "        [ 2.,  0.,  1.,  0.,  0.],\n",
      "        [12.,  0.,  2.,  1.,  1.],\n",
      "        [ 9.,  0.,  1.,  0.,  1.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.],\n",
      "        [ 9.,  0.,  1.,  1.,  1.],\n",
      "        [ 9.,  0.,  1.,  1.,  0.]])\n",
      "test_label_num_features:  tensor([6., 0., 1., 1., 0.])\n",
      "support_label_num_target:  tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "test_label_num_target:  tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "print(\"test_feature_item: \", test_feature_item)\n",
    "print(\"test_input: \", test_input)\n",
    "# print(\"support_feature_items: \", support_feature_items)\n",
    "# print(\"support_input: \", support_input)\n",
    "print(\"label_items: \", label_items)\n",
    "print(\"label_input: \", label_input)\n",
    "print(\"support_label_num_features: \", support_label_num_features)\n",
    "print(\"test_label_num_features: \", test_label_num_features)\n",
    "print(\"support_label_num_target: \", support_label_num_target)\n",
    "print(\"test_label_num_target: \", test_label_num_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OutputBuilder\n",
    "\n",
    "multi-label classification：还没有处理每个句子的labels；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputBuilderBase:\n",
    "    \"\"\"  Digitalizing the output targets\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, test_feature_item: FeatureItem, support_feature_items: FeatureItem,\n",
    "                 label2id: dict, max_support_size: int):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def pad_support_set(self, item_lst: List[List[int]], pad_value: int, max_support_size: int) -> List[List[int]]:\n",
    "        \"\"\"\n",
    "        pre-pad support set to insure: \n",
    "            1. each set has same sent num \n",
    "            2. each sent has same length\n",
    "        (do padding here because: \n",
    "            1. all support sent are considered as one tensor input  \n",
    "            2. support set size is small)\n",
    "        :param item_lst:\n",
    "        :param pad_value:\n",
    "        :param max_support_size:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        ''' pad sentences '''\n",
    "        max_sent_len = max([len(x) for x in item_lst])\n",
    "        ret = []\n",
    "        for sent in item_lst:\n",
    "            temp = sent[:]\n",
    "            while len(temp) < max_sent_len:\n",
    "                temp.append(pad_value)\n",
    "            ret.append(temp)\n",
    "        ''' pad support set size '''\n",
    "        pad_item = [pad_value for _ in range(max_sent_len)]\n",
    "        while len(ret) < max_support_size:\n",
    "            ret.append(pad_item)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FewShotOutputBuilder(OutputBuilderBase):\n",
    "    \"\"\"  Digitalizing the output targets as label id for non word piece tokens  \"\"\"\n",
    "    def __init__(self):\n",
    "        super(FewShotOutputBuilder, self).__init__()\n",
    "    \n",
    "    def __call__(self, test_feature_item: FeatureItem, \n",
    "                 support_feature_items: FeatureItem,\n",
    "                 label2id: dict, \n",
    "                 max_support_size: int):\n",
    "        test_target = self.item2label_ids(test_feature_item, label2id)\n",
    "        \n",
    "        # to estimate emission, the support target is one-hot here\n",
    "        support_target = [self.item2label_onehot(f_item, label2id) for f_item in support_feature_items]\n",
    "        \n",
    "        # padding为统一的shape\n",
    "        support_target = self.pad_support_set(support_target, self.label2onehot('[PAD]', label2id), max_support_size)\n",
    "        return torch.LongTensor(test_target), torch.LongTensor(support_target)\n",
    "\n",
    "    def item2label_ids(self, f_item: FeatureItem, label2id: dict):\n",
    "        return [label2id[lb] for lb in f_item.labels]\n",
    "\n",
    "    def item2label_onehot(self, f_item: FeatureItem, label2id: dict):\n",
    "        return [self.label2onehot(lb, label2id) for lb in f_item.labels]\n",
    "\n",
    "    def label2onehot(self, label: str, label2id: dict):\n",
    "        onehot = [0 for _ in range(len(label2id))]\n",
    "        onehot[label2id[label]] = 1\n",
    "        return onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeatureConstructor\n",
    "\n",
    "将预处理的流程集中起来；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureConstructor:\n",
    "    \"\"\"\n",
    "    Class for build feature and label2id dict\n",
    "    Main function:\n",
    "        construct_feature： 得到feature；\n",
    "        make_dict：收集标签集，得到标签的编号；\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_builder: InputBuilderBase,\n",
    "            output_builder: OutputBuilderBase,\n",
    "    ):\n",
    "        self.input_builder = input_builder\n",
    "        self.output_builder = output_builder\n",
    "\n",
    "    def construct_feature(\n",
    "            self,\n",
    "            examples: List[FewShotExample],\n",
    "            max_support_size: int,\n",
    "            label2id: dict,\n",
    "            id2label: dict,\n",
    "    ) -> List[FewShotFeature]:\n",
    "        all_features = []\n",
    "        for example in examples:\n",
    "            feature = self.example2feature(example, max_support_size, label2id, id2label)\n",
    "            all_features.append(feature)\n",
    "        return all_features\n",
    "\n",
    "    def example2feature(\n",
    "            self,\n",
    "            example: FewShotExample,\n",
    "            max_support_size: int,\n",
    "            label2id: dict,\n",
    "            id2label: dict\n",
    "    ) -> FewShotFeature:\n",
    "        test_feature_item, test_input, support_feature_items, support_input = self.input_builder(\n",
    "            example, max_support_size, label2id)\n",
    "        test_target, support_target = self.output_builder(\n",
    "            test_feature_item, support_feature_items, label2id, max_support_size)\n",
    "        ret = FewShotFeature(\n",
    "            gid=example.gid,\n",
    "            test_gid=example.test_id,\n",
    "            batch_gid=example.batch_id,\n",
    "            test_input=test_input,\n",
    "            test_feature_item=test_feature_item,\n",
    "            support_input=support_input,\n",
    "            support_feature_items=support_feature_items,\n",
    "            test_target=test_target,\n",
    "            support_target=support_target,\n",
    "        )\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelNumSchemaFeatureConstructor(FeatureConstructor):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_builder: InputBuilderBase,\n",
    "            output_builder: OutputBuilderBase,\n",
    "    ):\n",
    "        super(LabelNumSchemaFeatureConstructor, self).__init__(input_builder, output_builder)\n",
    "\n",
    "    def example2feature(\n",
    "            self,\n",
    "            example: FewShotExample,\n",
    "            max_support_size: int,\n",
    "            label2id: dict,\n",
    "            id2label: dict\n",
    "    ) -> FewShotFeature:\n",
    "        test_feature_item, test_input, support_feature_items, support_input, label_items, label_input, \\\n",
    "            support_label_num_features, test_label_num_features, support_label_num_target, test_label_num_target = \\\n",
    "            self.input_builder(example, max_support_size, label2id)\n",
    "        test_target, support_target = self.output_builder(\n",
    "            test_feature_item, support_feature_items, label2id, max_support_size)\n",
    "        ret = FewShotFeature(\n",
    "            gid=example.gid,\n",
    "            test_gid=example.test_id,\n",
    "            batch_gid=example.batch_id,\n",
    "            test_input=test_input,\n",
    "            test_feature_item=test_feature_item,\n",
    "            support_input=support_input,\n",
    "            support_feature_items=support_feature_items,\n",
    "            test_target=test_target,\n",
    "            support_target=support_target,\n",
    "            label_input=label_input,\n",
    "            label_items=label_items,\n",
    "            support_label_num_feature=support_label_num_features,\n",
    "            test_label_num_feature=test_label_num_features,\n",
    "            support_label_num_target=support_label_num_target,\n",
    "            test_label_num_target=test_label_num_target\n",
    "        )\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批量处理数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_feature(path):\n",
    "    with open(path, 'rb') as reader:\n",
    "        saved_feature = pickle.load(reader)\n",
    "        return saved_feature['features'], saved_feature['label2id'], saved_feature['id2label']\n",
    "\n",
    "\n",
    "def get_training_data_and_feature(opt, data_loader, preprocessor):\n",
    "    \"\"\" prepare feature and data \"\"\"\n",
    "    \n",
    "    # 如果有缓存：加载缓存\n",
    "    if opt.load_feature:\n",
    "        try:\n",
    "            train_features, train_label2id, train_id2label = load_feature(opt.train_path.replace('.json', '.saved.pk'))\n",
    "            dev_features, dev_label2id, dev_id2label = load_feature(opt.dev_path.replace('.json', '.saved.pk'))\n",
    "        except FileNotFoundError:\n",
    "            opt.load_feature, opt.save_feature = False, True  # Not a saved feature file yet, make it\n",
    "            train_features, train_label2id, train_id2label, dev_features, dev_label2id, dev_id2label =\\\n",
    "                get_training_data_and_feature(opt, data_loader, preprocessor)\n",
    "            opt.load_feature, opt.save_feature = True, False  # restore option\n",
    "    else:\n",
    "\n",
    "        # 从json文件中读出数据\n",
    "        logger.info(\"opt.train_path: {}\".format(opt.train_path))\n",
    "        train_examples, train_max_len, train_max_support_size = \\\n",
    "            data_loader.load_data(\n",
    "                path=opt.train_path\n",
    "            )\n",
    "        dev_examples, dev_max_len, dev_max_support_size = data_loader.load_data(path=opt.dev_path)\n",
    "\n",
    "        # 拿到label标签的编号\n",
    "        #   --> 因为现在是小样本学习，考察模型迁移能力，所以train和dev的标签体系是不一样的\n",
    "        train_label2id, train_id2label = make_dict(opt, train_examples)\n",
    "        dev_label2id, dev_id2label = make_dict(opt, dev_examples)\n",
    "        print(\"train_label2id: \\n\", train_label2id)\n",
    "        print(\"dev_label2id: \\n\", dev_label2id)\n",
    "        logger.info(' Finish train dev prepare dict ')\n",
    "\n",
    "        # 将文本和标签进行序列化；\n",
    "        train_features = preprocessor.construct_feature(\n",
    "            train_examples,\n",
    "            train_max_support_size,\n",
    "            train_label2id,\n",
    "            train_id2label\n",
    "        )\n",
    "        dev_features = preprocessor.construct_feature(\n",
    "            dev_examples,\n",
    "            dev_max_support_size,\n",
    "            dev_label2id,\n",
    "            dev_id2label\n",
    "        )\n",
    "        logger.info(' Finish prepare train dev features ')\n",
    "        \n",
    "        # 缓存至文件\n",
    "        if opt.save_feature:\n",
    "            save_feature(opt.train_path.replace('.json', '.saved.pk'), train_features, train_label2id, train_id2label)\n",
    "            save_feature(opt.dev_path.replace('.json', '.saved.pk'), dev_features, dev_label2id, dev_id2label)\n",
    "    return train_features, train_label2id, train_id2label, dev_features, dev_label2id, dev_id2label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_testing_data_feature(opt, data_loader, preprocessor):\n",
    "    \"\"\" prepare feature and data \"\"\"\n",
    "    if opt.load_feature:\n",
    "        try:\n",
    "            test_features, test_label2id, test_id2label = load_feature(opt.test_path.replace('.json', '.saved.pk'))\n",
    "        except FileNotFoundError:\n",
    "            opt.load_feature, opt.save_feature = False, True  # Not a saved feature file yet, make it\n",
    "            test_features, test_label2id, test_id2label = get_testing_data_feature(opt, data_loader, preprocessor)\n",
    "            opt.load_feature, opt.save_feature = True, False  # restore option\n",
    "    else:\n",
    "        test_examples, test_max_len, test_max_support_size = data_loader.load_data(path=opt.test_path)\n",
    "        test_label2id, test_id2label = make_dict(opt, test_examples)\n",
    "        logger.info(' Finish prepare test dict')\n",
    "        test_features = preprocessor.construct_feature(\n",
    "            test_examples, test_max_support_size, test_label2id, test_id2label)\n",
    "        logger.info(' Finish prepare test feature')\n",
    "        if opt.save_feature:\n",
    "            save_feature(opt.test_path.replace('.json', '.saved.pk'), test_features, test_label2id, test_id2label)\n",
    "    return test_features, test_label2id, test_id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMaJPtlMHur6DonwEyZLw5h",
   "collapsed_sections": [],
   "name": "data process and load.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
