{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**目录：**\n",
    "1. Trainer\n",
    "    - training：训练过程;\n",
    "        -- Trainer\n",
    "        \n",
    "    - 评估；\n",
    "        -- Tester\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 元学习参数名称\n",
    "\n",
    "upper_structures = [\n",
    "            'backoff',   # 与本文无关\n",
    "            'scale_rate',   # 来自 LearnableScaleController\n",
    "            'f_theta',     # TapNetSimilarityScorer\n",
    "            'phi',       # TapNetSimilarityScorer\n",
    "            'start_reps',   # 与本文无关\n",
    "            'end_reps',   # 与本文无关\n",
    "            'biaffine',     # 应该是旧版本\n",
    "            'threshold',      # threshold_type=learn的时候，这个参数可学习\n",
    "            'bandwidth',  # kernel regression \n",
    "            'map_linear'  # kernel regression \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_optimizer(opt, model, num_train_features, upper_structures=None):\n",
    "    \"\"\"\n",
    "    :param opt:\n",
    "    :param model:\n",
    "    :param num_train_features:\n",
    "    :param upper_structures: list of param name that use different learning rate.\n",
    "                These names should be unique sub-str.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    num_train_steps = int(\n",
    "        num_train_features / opt.train_batch_size / opt.gradient_accumulation_steps * opt.num_train_epochs)\n",
    "\n",
    "    ''' special process for space saving '''\n",
    "    if opt.fp16:\n",
    "        param_to_optimize = [(n, param.clone().detach().to('cpu').float().requires_grad_())\n",
    "                           for n, param in model.named_parameters()]\n",
    "    elif opt.optimize_on_cpu:\n",
    "        param_to_optimize = [(n, param.clone().detach().to('cpu').requires_grad_())\n",
    "                           for n, param in model.named_parameters()]\n",
    "    else:\n",
    "        param_to_optimize = list(model.named_parameters())  # all parameter name and parameter\n",
    "\n",
    "    ''' construct optimizer '''\n",
    "    if upper_structures and opt.upper_lr > 0:  # use different learning rate for upper structure parameter\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_to_optimize if not any(nd in n for nd in upper_structures)],\n",
    "             'weight_decay': 0.01, 'lr': opt.learning_rate},\n",
    "            {'params': [p for n, p in param_to_optimize if any(nd in n for nd in upper_structures)],\n",
    "             'weight_decay': 0.1, 'lr': opt.upper_lr},\n",
    "        ]\n",
    "    else:\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_to_optimize], 'weight_decay': 0.01, 'lr': opt.learning_rate},\n",
    "        ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=opt.learning_rate, correct_bias=False)\n",
    "\n",
    "    ''' construct scheduler '''\n",
    "    num_warmup_steps = int(opt.warmup_proportion * num_train_steps)\n",
    "    if opt.scheduler == 'linear_warmup':\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps)  # PyTorch scheduler\n",
    "    elif opt.scheduler == 'linear_decay':\n",
    "        if 0 < opt.decay_lr < 1:\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=opt.decay_epoch_size, gamma=opt.decay_lr)\n",
    "        else:\n",
    "            raise ValueError('illegal lr decay rate.')\n",
    "    else:\n",
    "        raise ValueError('Wrong scheduler')\n",
    "    return param_to_optimize, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerBase:\n",
    "    \"\"\"\n",
    "    Build a pytorch trainer, it is design to be:\n",
    "        - reusable for different training data\n",
    "        - reusable for different training model instance\n",
    "        - contains 2 model selection strategy:\n",
    "            - dev and test(optional) during training. (not suitable when the model is very large)\n",
    "            - store all checkpoint to disk.\n",
    "    Support features:\n",
    "        - multi-gpu [accelerating]\n",
    "        - distributed gpu [accelerating]\n",
    "        - 16bit-float training [save space]\n",
    "        - split batch [save space]\n",
    "        - model selection(dev & test) [better result & unexpected exit]\n",
    "        - check-point [unexpected exit]\n",
    "        - early stop [save time]\n",
    "        - padding when forward [better result & save space]\n",
    "        - grad clipping [better result]\n",
    "        - step learning rate decay [better result]\n",
    "    \"\"\"\n",
    "    def __init__(self, opt, optimizer, scheduler, param_to_optimize, device, n_gpu, tester=None):\n",
    "        \"\"\"\n",
    "        :param opt: args\n",
    "        :param optimizer:\n",
    "        :param scheduler:\n",
    "        :param param_to_optimize: model's params to optimize\n",
    "        :param device: torch class for training device,\n",
    "        :param n_gpu:  number of gpu used\n",
    "        :param tester: class for evaluation\n",
    "        \"\"\"\n",
    "        if opt.gradient_accumulation_steps < 1:\n",
    "            raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n",
    "                opt.gradient_accumulation_steps))\n",
    "\n",
    "        self.opt = opt\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.param_to_optimize = param_to_optimize\n",
    "        self.tester = tester  # for model selection, set 'None' to not select\n",
    "        self.gradient_accumulation_steps = opt.gradient_accumulation_steps\n",
    "        # Following is used to split the batch to save space\n",
    "        self.batch_size = int(opt.train_batch_size / opt.gradient_accumulation_steps)\n",
    "        self.device = device\n",
    "        self.n_gpu = n_gpu\n",
    "\n",
    "    def do_train(self, model, train_features, num_train_epochs,\n",
    "                 dev_features=None, dev_id2label=None,\n",
    "                 test_features=None, test_id2label=None,\n",
    "                 best_dev_score_now=0):\n",
    "        \"\"\"\n",
    "        do training and dev model selection\n",
    "        :param model:\n",
    "        :param train_features:\n",
    "        :param dev_features:\n",
    "        :param dev_id2label:\n",
    "        :param test_features:\n",
    "        :param test_id2label:\n",
    "        :param best_dev_score_now:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        num_train_steps = int(\n",
    "            len(train_features) / self.batch_size / self.gradient_accumulation_steps * num_train_epochs)\n",
    "\n",
    "        logger.info(\"***** Running training *****\")\n",
    "        logger.info(\"  Num features = %d\", len(train_features))\n",
    "        logger.info(\"  Batch size = %d\", self.batch_size)\n",
    "        logger.info(\"  Num steps = %d\", num_train_steps)\n",
    "        global_step = 0  # used for args.fp16\n",
    "        total_step = 0\n",
    "        best_dev_score_now = best_dev_score_now\n",
    "        best_model_now = model\n",
    "        test_score = None\n",
    "        min_loss = 100000000000000\n",
    "        loss_now = 0\n",
    "        no_new_best_dev_num = 0\n",
    "        no_loss_decay_steps = 0\n",
    "        is_convergence = False\n",
    "\n",
    "        model.train()\n",
    "        dataset = self.get_dataset(train_features)\n",
    "        sampler = self.get_sampler(dataset)\n",
    "        data_loader = self.get_data_loader(dataset, sampler)\n",
    "\n",
    "        # DEBUG\n",
    "        label_num_right_num = 0\n",
    "        label_num_all_num = 0\n",
    "\n",
    "        for epoch_id in trange(int(num_train_epochs), desc=\"Epoch\"):\n",
    "            for step, batch in enumerate(tqdm(data_loader, desc=\"Train-Batch Progress\")):\n",
    "                if self.n_gpu == 1:\n",
    "                    batch = tuple(t.to(self.device) for t in batch)  # multi-gpu does scattering it-self\n",
    "                \n",
    "                ''' loss '''\n",
    "                # 一次前向传播\n",
    "                loss = self.do_forward(batch, model, epoch_id, step)\n",
    "                # 模型 并行， fp16， 梯度累积 的处理\n",
    "                loss = self.process_special_loss(loss)  # for parallel process, split batch and so on\n",
    "                loss.backward()\n",
    "\n",
    "                # DEBUG\n",
    "                if self.opt.decoder == 'krnmsmlc':\n",
    "                    label_num_right_num += torch.sum(model.decoder.right_estimate).item()\n",
    "                    label_num_all_num += self.batch_size\n",
    "\n",
    "                ''' optimizer step '''\n",
    "                # 一步参数更新\n",
    "                global_step, model, is_nan, update_model = self.optimizer_step(step, model, global_step)\n",
    "                if is_nan:  # FP16 TRAINING: Nan in gradients, reducing loss scaling\n",
    "                    continue\n",
    "                total_step += 1\n",
    "                \n",
    "                # 模型打分，更新最优模型\n",
    "                ''' model selection '''\n",
    "                if self.time_to_make_check_point(total_step, data_loader):\n",
    "                    if self.tester and self.opt.eval_when_train:  # this is not suit for training big model\n",
    "                        print(\"Start dev eval.\")\n",
    "                        dev_score, test_score, copied_best_model = self.model_selection(\n",
    "                            model, best_dev_score_now, dev_features, dev_id2label, test_features, test_id2label)\n",
    "\n",
    "                        if dev_score > best_dev_score_now:\n",
    "                            best_dev_score_now = dev_score\n",
    "                            best_model_now = copied_best_model\n",
    "                            no_new_best_dev_num = 0\n",
    "                        else:\n",
    "                            no_new_best_dev_num += 1\n",
    "                    else:\n",
    "                        self.make_check_point_(model=model, step=total_step)\n",
    "                \n",
    "                #  模型训练的早停\n",
    "                ''' convergence detection & early stop '''\n",
    "                loss_now = loss.item() if update_model else loss.item() + loss_now\n",
    "                if self.opt.convergence_window > 0 and update_model:\n",
    "                    if global_step % 100 == 0 or total_step % len(data_loader) == 0:\n",
    "                        print('Current loss {}, global step {}, min loss now {}, no loss decay step {}'.format(\n",
    "                            loss_now, global_step, min_loss, no_loss_decay_steps))\n",
    "                    if loss_now < min_loss:\n",
    "                        min_loss = loss_now\n",
    "                        no_loss_decay_steps = 0\n",
    "                    else:\n",
    "                        no_loss_decay_steps += 1\n",
    "                        if no_loss_decay_steps >= self.opt.convergence_window:\n",
    "                            logger.info('=== Reach convergence point!!!!!! ====')\n",
    "                            print('=== Reach convergence point!!!!!! ====')\n",
    "                            is_convergence = True\n",
    "                if no_new_best_dev_num >= self.opt.convergence_dev_num > 0:\n",
    "                    logger.info('=== Reach convergence point!!!!!! ====')\n",
    "                    print('=== Reach convergence point!!!!!! ====')\n",
    "                    is_convergence = True\n",
    "                if is_convergence:\n",
    "                    break\n",
    "            if is_convergence:\n",
    "                break\n",
    "            print(\" --- The {} epoch Finish --- \".format(epoch_id))\n",
    "\n",
    "        if self.opt.decoder == 'krnmsmlc':\n",
    "            print('DEBUG: label num estimate right rate {} / {} = {}'.format(\n",
    "                label_num_right_num, label_num_all_num, label_num_right_num / label_num_all_num))\n",
    "\n",
    "        return best_model_now, best_dev_score_now, test_score\n",
    "\n",
    "    def time_to_make_check_point(self, step, data_loader):\n",
    "        interval_size = int(len(data_loader) / self.opt.cpt_per_epoch)\n",
    "        remained_step = len(data_loader) - (step % len(data_loader))  # remained step for current epoch\n",
    "        return (step % interval_size == 0 < interval_size <= remained_step) or (step % len(data_loader) == 0)\n",
    "\n",
    "    def get_dataset(self, features):\n",
    "        return TensorDataset([self.unpack_feature(f) for f in features])\n",
    "\n",
    "    def get_sampler(self, dataset):\n",
    "        if self.opt.local_rank == -1:\n",
    "            sampler = RandomSampler(dataset)\n",
    "        else:\n",
    "            sampler = DistributedSampler(dataset)\n",
    "        return sampler\n",
    "\n",
    "    def get_data_loader(self, dataset, sampler):\n",
    "        data_loader = DataLoader(dataset, sampler=sampler, batch_size=self.batch_size)\n",
    "        return data_loader\n",
    "\n",
    "    def process_special_loss(self, loss):\n",
    "        if self.n_gpu > 1:\n",
    "            loss = loss.mean()  # mean() to average on multi-gpu.\n",
    "        if self.opt.fp16 and self.opt.loss_scale != 1.0:\n",
    "            # rescale loss for fp16 training\n",
    "            # see https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html\n",
    "            loss = loss * self.opt.loss_scale\n",
    "        if self.opt.gradient_accumulation_steps > 1:\n",
    "            loss = loss / self.opt.gradient_accumulation_steps\n",
    "        return loss\n",
    "\n",
    "    def set_optimizer_params_grad(self, param_to_optimize, named_params_model, test_nan=False):\n",
    "        \"\"\" Utility function for optimize_on_cpu and 16-bits training.\n",
    "            Copy the gradient of the GPU parameters to the CPU/RAMM copy of the model\n",
    "        \"\"\"\n",
    "        is_nan = False\n",
    "        for (name_opti, param_opti), (name_model, param_model) in zip(param_to_optimize, named_params_model):\n",
    "            if name_opti != name_model:\n",
    "                logger.error(\"name_opti != name_model: {} {}\".format(name_opti, name_model))\n",
    "                raise ValueError\n",
    "            if param_model.grad is not None:\n",
    "                if test_nan and torch.isnan(param_model.grad).sum() > 0:\n",
    "                    is_nan = True\n",
    "                if param_opti.grad is None:\n",
    "                    param_opti.grad = torch.nn.Parameter(param_opti.data.new().resize_(*param_opti.data.size()))\n",
    "                param_opti.grad.data.copy_(param_model.grad.data)\n",
    "            else:\n",
    "                param_opti.grad = None\n",
    "        return is_nan\n",
    "\n",
    "    def copy_optimizer_params_to_model(self, named_params_model, named_params_optimizer):\n",
    "        \"\"\" Utility function for optimize_on_cpu and 16-bits training.\n",
    "            Copy the parameters optimized on CPU/RAM back to the model on GPU\n",
    "        \"\"\"\n",
    "        for (name_opti, param_opti), (name_model, param_model) in zip(named_params_optimizer, named_params_model):\n",
    "            if name_opti != name_model:\n",
    "                logger.error(\"name_opti != name_model: {} {}\".format(name_opti, name_model))\n",
    "                raise ValueError\n",
    "            param_model.data.copy_(param_opti.data)\n",
    "\n",
    "    def make_check_point(self, model, step):\n",
    "        logger.info(\"Save model check point to file:%s\", os.path.join(\n",
    "            self.opt.output_dir, 'model.step{}.cpt.pl'.format(step)))\n",
    "        torch.save(\n",
    "            self.check_point_content(model), os.path.join(self.opt.output_dir, 'model.step{}.cpt.pl'.format(step)))\n",
    "\n",
    "    def make_check_point_(self, model, step):\n",
    "        \"\"\" deal with IO error version \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Save model check point to file:%s\", os.path.join(\n",
    "                self.opt.output_dir, 'model.step{}.cpt.pl'.format(step)))\n",
    "            torch.save(\n",
    "                self.check_point_content(model), os.path.join(self.opt.output_dir, 'model.step{}.cpt.pl'.format(step)))\n",
    "        except IOError:\n",
    "            logger.info(\"Failed to make cpt, sleeping ...\")\n",
    "            time.sleep(300)\n",
    "            self.make_check_point_(model, step)\n",
    "\n",
    "    def model_selection(self, model, best_score, dev_features, dev_id2label, test_features=None, test_id2label=None):\n",
    "        \"\"\" do model selection during training\"\"\"\n",
    "        print(\"Start dev model selection.\")\n",
    "        # do dev eval at every dev_interval point and every end of epoch\n",
    "        dev_model = self.tester.clone_model(model, dev_id2label)  # copy reusable params, for a different domain\n",
    "        \n",
    "#         if self.opt.mask_transition and self.opt.task == 'sl':\n",
    "#             dev_model.label_mask = self.opt.dev_label_mask.to(self.device)\n",
    "        \n",
    "        dev_score = self.tester.do_test(dev_model, dev_features, dev_id2label, log_mark='dev_pred')\n",
    "        logger.info(\"  dev score(F1) = {}\".format(dev_score))\n",
    "        print(\"  dev score(F1) = {}\".format(dev_score))\n",
    "        best_model = None\n",
    "        test_score = None\n",
    "        if dev_score > best_score:\n",
    "            logger.info(\" === Found new best!! === \")\n",
    "            ''' store new best model  '''\n",
    "            best_model = self.clone_model(model)  # copy model to avoid writen by latter training\n",
    "            ''' save model file '''\n",
    "            logger.info(\"Save model to file:%s\", os.path.join(self.opt.output_dir, 'model.pl'))\n",
    "            torch.save(self.check_point_content(model), os.path.join(self.opt.output_dir, 'model.pl'))\n",
    "\n",
    "            ''' get current best model's test score '''\n",
    "            # 也做一遍测试\n",
    "            if test_features:\n",
    "                test_model = self.tester.clone_model(model, test_id2label)  # copy reusable params for different domain\n",
    "                if self.opt.mask_transition and self.opt.task == 'sl':\n",
    "                    test_model.label_mask = self.opt.test_label_mask.to(self.device)\n",
    "                test_score = self.tester.do_test(test_model, test_features, test_id2label, log_mark='test_pred')\n",
    "                logger.info(\"  test score(F1) = {}\".format(test_score))\n",
    "                print(\"  test score(F1) = {}\".format(test_score))\n",
    "        # reset the model status\n",
    "        model.train()\n",
    "        return dev_score, test_score, best_model\n",
    "\n",
    "    def check_point_content(self, model):\n",
    "        \"\"\" necessary staff for rebuild the model \"\"\"\n",
    "        model = model\n",
    "        # model = model if self.n_gpu <= 1 else model.module\n",
    "        return model.state_dict()\n",
    "\n",
    "    def select_model_from_check_point(\n",
    "            self, train_id2label, dev_features, dev_id2label, test_features=None, test_id2label=None, rm_cpt=True):\n",
    "        all_cpt_file = list(filter(lambda x: '.cpt.pl' in x, os.listdir(self.opt.output_dir)))\n",
    "        best_score = 0\n",
    "        test_score_then = 0\n",
    "        best_model = None\n",
    "        all_cpt_file = sorted(all_cpt_file, key=lambda x: int(x.replace('model.step', '').replace('.cpt.pl', '')))\n",
    "        for cpt_file in all_cpt_file:\n",
    "            logger.info('testing check point: {}'.format(cpt_file))\n",
    "            model = load_model(os.path.join(self.opt.output_dir, cpt_file))\n",
    "            dev_score, test_score, copied_model = self.model_selection(\n",
    "                model, best_score, dev_features, dev_id2label, test_features, test_id2label)\n",
    "            if dev_score > best_score:\n",
    "                best_score = dev_score\n",
    "                test_score_then = test_score\n",
    "                best_model = copied_model\n",
    "        if rm_cpt:  # delete all check point\n",
    "            for cpt_file in all_cpt_file:\n",
    "                os.unlink(os.path.join(self.opt.output_dir, cpt_file))\n",
    "        return best_model, best_score, test_score_then\n",
    "\n",
    "    def unpack_feature(self, feature) -> List[torch.Tensor]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def clone_model(self, model):\n",
    "        # get a new instance\n",
    "        return copy.deepcopy(model)\n",
    "\n",
    "    def do_forward(self, batch, model, epoch_id, step):\n",
    "        loss = model(*batch)\n",
    "        return loss\n",
    "\n",
    "    def optimizer_step(self, step, model, global_step):\n",
    "        is_nan = False\n",
    "        update_model = False\n",
    "        if (step + 1) % self.gradient_accumulation_steps == 0:  # for both memory saving setting and normal setting\n",
    "            if self.opt.clip_grad > 0:\n",
    "                torch.nn.utils.clip_grad_value_(model.parameters(), self.opt.clip_grad)\n",
    "            if self.opt.fp16 or self.opt.optimize_on_cpu:\n",
    "                if self.opt.fp16 and self.opt.loss_scale != 1.0:\n",
    "                    # scale down gradients for fp16 training\n",
    "                    for param in model.parameters():\n",
    "                        if param.grad is not None:\n",
    "                            param.grad.data = param.grad.data / self.opt.loss_scale\n",
    "                is_nan = self.set_optimizer_params_grad(self.param_to_optimize, model.named_parameters(), test_nan=True)\n",
    "                if is_nan:\n",
    "                    logger.info(\"FP16 TRAINING: Nan in gradients, reducing loss scaling\")\n",
    "                    self.opt.loss_scale = self.opt.loss_scale / 2\n",
    "                    model.zero_grad()\n",
    "                    return global_step, model, is_nan\n",
    "                self.optimizer.step()\n",
    "                self.copy_optimizer_params_to_model(model.named_parameters(), self.param_to_optimize)\n",
    "            else:\n",
    "                self.optimizer.step()\n",
    "            if self.scheduler:  # decay learning rate\n",
    "                self.scheduler.step()\n",
    "            model.zero_grad()\n",
    "            global_step += 1\n",
    "            update_model = True\n",
    "        return global_step, model, is_nan, update_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FewShotTrainer(TrainerBase):\n",
    "    \"\"\"\n",
    "    Support features:\n",
    "        - multi-gpu [accelerating]\n",
    "        - distributed gpu [accelerating]\n",
    "        - 16bit-float training [save space]\n",
    "        - split batch [save space]\n",
    "        - model selection(dev & test) [better result & unexpected exit]\n",
    "        - check-point [unexpected exit]\n",
    "        - early stop [save time]\n",
    "        - padding when forward [better result & save space]\n",
    "    \"\"\"\n",
    "    def __init__(self, opt, optimizer, scheduler, param_to_optimize, device, n_gpu, tester=None):\n",
    "        super(FewShotTrainer, self).__init__(opt, optimizer, scheduler, param_to_optimize, device, n_gpu, tester)\n",
    "\n",
    "    def get_dataset(self, features):\n",
    "        return FewShotDataset([self.unpack_feature(f) for f in features])\n",
    "\n",
    "    def get_sampler(self, dataset):\n",
    "        # 应对不同的训练环境\n",
    "        if self.opt.local_rank == -1:\n",
    "            if self.opt.sampler_type == 'similar_len':\n",
    "                sampler = SimilarLengthSampler(dataset, batch_size=self.batch_size)\n",
    "            elif self.opt.sampler_type == 'random':\n",
    "                sampler = RandomSampler(dataset)\n",
    "            else:\n",
    "                raise TypeError('the sampler_type is not true')\n",
    "        else:\n",
    "            sampler = DistributedSampler(dataset)\n",
    "        return sampler\n",
    "\n",
    "    def get_data_loader(self, dataset, sampler):\n",
    "        # PadCollate： pads according to the longest sequence in a batch of sequences\n",
    "        pad_collate = PadCollate(dim=-1, sp_dim=-2, sp_item_idx=[3, 8, 12])  # nwp_index, spt_tgt need special padding\n",
    "        \n",
    "        data_loader = DataLoader(dataset, sampler=sampler, batch_size=self.batch_size, collate_fn=pad_collate)\n",
    "        return data_loader\n",
    "\n",
    "    def unpack_feature(self, feature: FewShotFeature) -> List[torch.Tensor]:\n",
    "        ret = [\n",
    "            torch.LongTensor([feature.gid]),\n",
    "            # test\n",
    "            feature.test_input.token_ids,\n",
    "            feature.test_input.segment_ids,\n",
    "            feature.test_input.nwp_index,\n",
    "            feature.test_input.input_mask,\n",
    "            feature.test_input.output_mask,\n",
    "            # support\n",
    "            feature.support_input.token_ids,\n",
    "            feature.support_input.segment_ids,\n",
    "            feature.support_input.nwp_index,\n",
    "            feature.support_input.input_mask,\n",
    "            feature.support_input.output_mask,\n",
    "            # target\n",
    "            feature.test_target,\n",
    "            feature.support_target,\n",
    "            # Special\n",
    "            torch.LongTensor([len(feature.support_feature_items)]),  # support num\n",
    "        ]\n",
    "        return ret\n",
    "\n",
    "    def do_forward(self, batch, model, epoch_id, step):\n",
    "        (\n",
    "            gid,  # 0\n",
    "            test_token_ids,  # 1\n",
    "            test_segment_ids,  # 2\n",
    "            test_nwp_index,  # 3\n",
    "            test_input_mask,  # 4\n",
    "            test_output_mask,  # 5\n",
    "            support_token_ids,  # 6\n",
    "            support_segment_ids,  # 7\n",
    "            support_nwp_index,  # 8\n",
    "            support_input_mask,  # 9\n",
    "            support_output_mask,  # 10\n",
    "            test_target,  # 11\n",
    "            support_target,  # 12\n",
    "            support_num,  # 13\n",
    "        ) = batch\n",
    "\n",
    "        loss = model(\n",
    "            test_token_ids,\n",
    "            test_segment_ids,\n",
    "            test_nwp_index,\n",
    "            test_input_mask,\n",
    "            test_output_mask,\n",
    "            support_token_ids,\n",
    "            support_segment_ids,\n",
    "            support_nwp_index,\n",
    "            support_input_mask,\n",
    "            support_output_mask,\n",
    "            test_target,\n",
    "            support_target,\n",
    "            support_num,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def check_point_content(self, model):\n",
    "        \"\"\" save staff for rebuild a model \"\"\"\n",
    "        model = model  # save sub-module may cause issues\n",
    "        sub_model = model if self.n_gpu <= 1 else model.module\n",
    "        ret = {\n",
    "            'state_dict': model.state_dict(),\n",
    "            'opt': self.opt,\n",
    "            'config': model.config,\n",
    "        }\n",
    "        return ret\n",
    "\n",
    "    def get_value_from_order_dict(self, order_dict, key):\n",
    "        \"\"\"\"\"\"\n",
    "        for k, v in order_dict.items():\n",
    "            if key in k:\n",
    "                return v\n",
    "        return []\n",
    "\n",
    "    def clone_model(self, model):\n",
    "        # deal with data parallel model\n",
    "        best_model: FewShotTextClassifier\n",
    "        old_model: FewShotTextClassifier\n",
    "        if self.opt.local_rank != -1 or self.n_gpu > 1:  # the model is parallel class here\n",
    "            old_model = model.module\n",
    "        else:\n",
    "            old_model = model\n",
    "        # get a new instance for different domain (cpu version to save resource)\n",
    "        config = {'num_tags': old_model.config['num_tags']}\n",
    "        if 'num_anchors' in old_model.config:\n",
    "            config['num_anchors'] = old_model.config['num_anchors']  # Use previous model's random anchors.\n",
    "        best_model = make_model(opt=old_model.opt, config=config)\n",
    "        # copy weights and stuff\n",
    "        best_model.load_state_dict(old_model.state_dict())\n",
    "        return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  可以看到与 FewShotTrainer 的区别在于引入了 label feature\n",
    "\n",
    "class SchemaFewShotTrainer(FewShotTrainer):\n",
    "    def __init__(self, opt, optimizer, scheduler, param_to_optimize, device, n_gpu, tester=None):\n",
    "        super(SchemaFewShotTrainer, self).__init__(opt, optimizer, scheduler, param_to_optimize, device, n_gpu, tester)\n",
    "\n",
    "    def get_data_loader(self, dataset, sampler):\n",
    "        \"\"\" add label index into special padding \"\"\"\n",
    "        pad_collate = PadCollate(dim=-1, sp_dim=-2, sp_item_idx=[3, 8, 12, 16])  # nwp_index, spt_tgt need sp-padding\n",
    "        data_loader = DataLoader(dataset, sampler=sampler, batch_size=self.batch_size, collate_fn=pad_collate)\n",
    "        return data_loader\n",
    "\n",
    "    def unpack_feature(self, feature: FewShotFeature) -> List[torch.Tensor]:\n",
    "        ret = [\n",
    "            torch.LongTensor([feature.gid]),\n",
    "            # test\n",
    "            feature.test_input.token_ids,\n",
    "            feature.test_input.segment_ids,\n",
    "            feature.test_input.nwp_index,\n",
    "            feature.test_input.input_mask,\n",
    "            feature.test_input.output_mask,\n",
    "            # support\n",
    "            feature.support_input.token_ids,\n",
    "            feature.support_input.segment_ids,\n",
    "            feature.support_input.nwp_index,\n",
    "            feature.support_input.input_mask,\n",
    "            feature.support_input.output_mask,\n",
    "            # target\n",
    "            feature.test_target,\n",
    "            feature.support_target,\n",
    "            # Special\n",
    "            torch.LongTensor([len(feature.support_feature_items)]),  # support num\n",
    "            # label feature\n",
    "            feature.label_input.token_ids,\n",
    "            feature.label_input.segment_ids,\n",
    "            feature.label_input.nwp_index,\n",
    "            feature.label_input.input_mask,\n",
    "            feature.label_input.output_mask,\n",
    "        ]\n",
    "        return ret\n",
    "\n",
    "    def do_forward(self, batch, model, epoch_id, step):\n",
    "        (\n",
    "            gid,  # 0\n",
    "            test_token_ids,  # 1\n",
    "            test_segment_ids,  # 2\n",
    "            test_nwp_index,  # 3\n",
    "            test_input_mask,  # 4\n",
    "            test_output_mask,  # 5\n",
    "            support_token_ids,  # 6\n",
    "            support_segment_ids,  # 7\n",
    "            support_nwp_index,  # 8\n",
    "            support_input_mask,  # 9\n",
    "            support_output_mask,  # 10\n",
    "            test_target,  # 11\n",
    "            support_target,  # 12\n",
    "            support_num,  # 13\n",
    "            # label feature\n",
    "            label_token_ids,  # 14\n",
    "            label_segment_ids,  # 15\n",
    "            label_nwp_index,  # 16\n",
    "            label_input_mask,  # 17\n",
    "            label_output_mask,  # 18\n",
    "        ) = batch\n",
    "\n",
    "        loss = model(\n",
    "            # loss, prediction = model(\n",
    "            test_token_ids,\n",
    "            test_segment_ids,\n",
    "            test_nwp_index,\n",
    "            test_input_mask,\n",
    "            test_output_mask,\n",
    "            support_token_ids,\n",
    "            support_segment_ids,\n",
    "            support_nwp_index,\n",
    "            support_input_mask,\n",
    "            support_output_mask,\n",
    "            test_target,\n",
    "            support_target,\n",
    "            support_num,\n",
    "            # label feature\n",
    "            label_token_ids,\n",
    "            label_segment_ids,\n",
    "            label_nwp_index,\n",
    "            label_input_mask,\n",
    "            label_output_mask,\n",
    "        )\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进一步引入句子特征，用于句子的基于kernelregression的标签数量预测\n",
    "\n",
    "class LabelNumSchemaFewShotTrainer(SchemaFewShotTrainer):\n",
    "    def __init__(self, opt, optimizer, scheduler, param_to_optimize, device, n_gpu, tester=None):\n",
    "        super(LabelNumSchemaFewShotTrainer, self).__init__(opt, optimizer, scheduler, param_to_optimize, device, n_gpu, tester)\n",
    "\n",
    "    def get_data_loader(self, dataset, sampler):\n",
    "        \"\"\" add label index into special padding \"\"\"\n",
    "        pad_collate = PadCollate(\n",
    "            dim=-1,\n",
    "            sp_dim=-2,\n",
    "            sp_item_idx=[3, 8, 12, 16, 19]\n",
    "        )  # nwp_index, spt_tgt need sp-padding\n",
    "        data_loader = DataLoader(dataset, sampler=sampler, batch_size=self.batch_size, collate_fn=pad_collate)\n",
    "        return data_loader\n",
    "\n",
    "    def unpack_feature(self, feature: FewShotFeature) -> List[torch.Tensor]:\n",
    "        ret = [\n",
    "            torch.LongTensor([feature.gid]),\n",
    "            # test\n",
    "            feature.test_input.token_ids,\n",
    "            feature.test_input.segment_ids,\n",
    "            feature.test_input.nwp_index,\n",
    "            feature.test_input.input_mask,\n",
    "            feature.test_input.output_mask,\n",
    "            # support\n",
    "            feature.support_input.token_ids,\n",
    "            feature.support_input.segment_ids,\n",
    "            feature.support_input.nwp_index,\n",
    "            feature.support_input.input_mask,\n",
    "            feature.support_input.output_mask,\n",
    "            # target\n",
    "            feature.test_target,\n",
    "            feature.support_target,\n",
    "            # Special\n",
    "            torch.LongTensor([len(feature.support_feature_items)]),  # support num\n",
    "            # label feature\n",
    "            feature.label_input.token_ids,\n",
    "            feature.label_input.segment_ids,\n",
    "            feature.label_input.nwp_index,\n",
    "            feature.label_input.input_mask,\n",
    "            feature.label_input.output_mask,\n",
    "            # sentence feature\n",
    "            feature.support_label_num_feature,  # 19\n",
    "            feature.test_label_num_feature,  # 20\n",
    "            feature.support_label_num_target,  # 21\n",
    "            feature.test_label_num_target  # 22\n",
    "        ]\n",
    "        return ret\n",
    "\n",
    "    def do_forward(self, batch, model, epoch_id, step):\n",
    "        (\n",
    "            gid,  # 0\n",
    "            test_token_ids,  # 1\n",
    "            test_segment_ids,  # 2\n",
    "            test_nwp_index,  # 3\n",
    "            test_input_mask,  # 4\n",
    "            test_output_mask,  # 5\n",
    "            support_token_ids,  # 6\n",
    "            support_segment_ids,  # 7\n",
    "            support_nwp_index,  # 8\n",
    "            support_input_mask,  # 9\n",
    "            support_output_mask,  # 10\n",
    "            test_target,  # 11\n",
    "            support_target,  # 12\n",
    "            support_num,  # 13\n",
    "            # label feature\n",
    "            label_token_ids,  # 14\n",
    "            label_segment_ids,  # 15\n",
    "            label_nwp_index,  # 16\n",
    "            label_input_mask,  # 17\n",
    "            label_output_mask,  # 18\n",
    "            # sentence feature\n",
    "            support_label_num_feature,  # 19\n",
    "            test_label_num_feature,  # 20\n",
    "            support_label_num_target,  # 21\n",
    "            test_label_num_target  # 22\n",
    "        ) = batch\n",
    "\n",
    "        loss = model(\n",
    "            # loss, prediction = model(\n",
    "            test_token_ids,\n",
    "            test_segment_ids,\n",
    "            test_nwp_index,\n",
    "            test_input_mask,\n",
    "            test_output_mask,\n",
    "            support_token_ids,\n",
    "            support_segment_ids,\n",
    "            support_nwp_index,\n",
    "            support_input_mask,\n",
    "            support_output_mask,\n",
    "            test_target,\n",
    "            support_target,\n",
    "            support_num,\n",
    "            # label feature\n",
    "            label_token_ids,\n",
    "            label_segment_ids,\n",
    "            label_nwp_index,\n",
    "            label_input_mask,\n",
    "            label_output_mask,\n",
    "            # sentence feature\n",
    "            support_label_num_feature,\n",
    "            test_label_num_feature,\n",
    "            support_label_num_target,\n",
    "            test_label_num_target\n",
    "        )\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tester\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RawResult = collections.namedtuple(\"RawResult\", [\"feature\", \"prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TesterBase:\n",
    "    \"\"\"\n",
    "    Support features:\n",
    "        - multi-gpu [accelerating]\n",
    "        - distributed gpu [accelerating]\n",
    "        - padding when forward [better result & save space]\n",
    "    \"\"\"\n",
    "    def __init__(self, opt, device, n_gpu):\n",
    "        if opt.gradient_accumulation_steps < 1:\n",
    "            raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n",
    "                opt.gradient_accumulation_steps))\n",
    "\n",
    "        self.opt = opt\n",
    "        # Following is used to split the batch to save space\n",
    "        self.batch_size = opt.test_batch_size\n",
    "        self.device = device\n",
    "        self.n_gpu = n_gpu\n",
    "\n",
    "    def do_test(self, model: torch.nn.Module, test_features: List[FewShotFeature], id2label: dict,\n",
    "                log_mark: str = 'test_pred'):\n",
    "        logger.info(\"***** Running eval *****\")\n",
    "        logger.info(\"  Num features = %d\", len(test_features))\n",
    "        logger.info(\"  Batch size = %d\", self.batch_size)\n",
    "        all_results = []\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        data_loader = self.get_data_loader(test_features)\n",
    "\n",
    "        # DEBUG\n",
    "        label_num_right_num = 0\n",
    "        label_num_all_num = 0\n",
    "        \n",
    "        # 一个batch一个batch的得到预测结果\n",
    "        for batch in tqdm(data_loader, desc=\"Eval-Batch Progress\"):\n",
    "            batch = tuple(t.to(self.device) for t in batch)  # multi-gpu does scattering it-self\n",
    "            with torch.no_grad():\n",
    "                predictions = self.do_forward(batch, model)\n",
    "                # DEBUG\n",
    "                if self.opt.decoder == 'krnmsmlc':\n",
    "                    label_num_right_num += torch.sum(model.decoder.right_estimate).item()\n",
    "                    label_num_all_num += self.batch_size\n",
    "            \n",
    "            for i, feature_gid in enumerate(batch[0]):  # iter over feature global id\n",
    "                prediction = predictions[i]\n",
    "                feature = test_features[feature_gid.item()]\n",
    "                all_results.append(RawResult(feature=feature, prediction=prediction))\n",
    "                if model.emb_log:\n",
    "                    model.emb_log.write('text_' + str(feature_gid.item()) + '\\t'\n",
    "                                        + '\\t'.join(feature.test_feature_item.data_item.seq_in) + '\\n')\n",
    "\n",
    "        if self.opt.decoder == 'krnmsmlc':\n",
    "            print('DEBUG: label num estimate right rate {} / {} = {}'.format(\n",
    "                label_num_right_num, label_num_all_num, label_num_right_num / label_num_all_num))\n",
    "\n",
    "        # close file handler\n",
    "        if model.emb_log:\n",
    "            model.emb_log.close()\n",
    "\n",
    "        scores = self.eval_predictions(all_results, id2label, log_mark)\n",
    "        return scores\n",
    "\n",
    "    def get_data_loader(self, features):\n",
    "        dataset = TensorDataset([self.unpack_feature(f) for f in features])\n",
    "        if self.opt.local_rank == -1:\n",
    "            sampler = RandomSampler(dataset)\n",
    "        else:\n",
    "            sampler = DistributedSampler(dataset)\n",
    "        data_loader = DataLoader(dataset, sampler=sampler, batch_size=self.batch_size)\n",
    "        return data_loader\n",
    "\n",
    "    def clone_model(self, model, id2label):\n",
    "        # get a new instance\n",
    "        return copy.deepcopy(model)\n",
    "\n",
    "    def unpack_feature(self, feature) -> List[torch.Tensor]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def do_forward(self, batch, model):\n",
    "        prediction = model(*batch)\n",
    "        return prediction\n",
    "\n",
    "    def eval_predictions(self, *args, **kwargs) -> float:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FewShotTester(TesterBase):\n",
    "    \"\"\"\n",
    "        Support features:\n",
    "            - multi-gpu [accelerating]\n",
    "            - distributed gpu [accelerating]\n",
    "            - padding when forward [better result & save space]\n",
    "    \"\"\"\n",
    "    def __init__(self, opt, device, n_gpu):\n",
    "        super(FewShotTester, self).__init__(opt, device, n_gpu)\n",
    "\n",
    "    def get_data_loader(self, features):\n",
    "        # 与trainer一样\n",
    "        dataset = FewShotDataset([self.unpack_feature(f) for f in features])\n",
    "        if self.opt.local_rank == -1:\n",
    "            sampler = SequentialSampler(dataset)\n",
    "        else:\n",
    "            sampler = DistributedSampler(dataset)\n",
    "        pad_collate = PadCollate(dim=-1, sp_dim=-2, sp_item_idx=[3, 8, 12])  # nwp_index, spt_tgt need special padding\n",
    "        data_loader = DataLoader(dataset, sampler=sampler, batch_size=self.batch_size, collate_fn=pad_collate)\n",
    "        return data_loader\n",
    "\n",
    "    def eval_predictions(self, all_results: List[RawResult], id2label: dict, log_mark: str) -> float:\n",
    "        \"\"\" Our result score is average score of all few-shot batches. \"\"\"\n",
    "        # 测试表现就是全部batch的平均分\n",
    "        \n",
    "        # 拿到few-shot batch id 到 result的映射\n",
    "        all_batches = self.reform_few_shot_batch(all_results)\n",
    "        all_scores = []\n",
    "        for b_id, fs_batch in all_batches:\n",
    "            f1 = self.eval_one_few_shot_batch(b_id, fs_batch, id2label, log_mark)\n",
    "            all_scores.append(f1)\n",
    "        return sum(all_scores) * 1.0 / len(all_scores)\n",
    "\n",
    "    def eval_one_few_shot_batch(self, b_id, fs_batch: List[RawResult], id2label: dict, log_mark: str) -> float:\n",
    "        pred_file_name = '{}.{}.txt'.format(log_mark, b_id)\n",
    "        output_prediction_file = os.path.join(self.opt.output_dir, pred_file_name)\n",
    "        if self.opt.task == 'sl':\n",
    "            self.writing_sl_prediction(fs_batch, output_prediction_file, id2label)\n",
    "            precision, recall, f1 = self.eval_with_script(output_prediction_file)\n",
    "        elif self.opt.task == 'mlc':\n",
    "            precision, recall, f1 = self.writing_mlc_prediction(fs_batch, output_prediction_file, id2label)\n",
    "        elif self.opt.task == 'sc':\n",
    "            precision, recall, f1 = self.writing_sc_prediction(fs_batch, output_prediction_file, id2label)\n",
    "        else:\n",
    "            raise ValueError(\"Wrong task.\")\n",
    "        return f1\n",
    "\n",
    "    def writing_sc_prediction(self, fs_batch: List[RawResult], output_prediction_file: str, id2label: dict):\n",
    "        tp, fp, fn = 0, 0, 0\n",
    "        writing_content = []\n",
    "        for result in fs_batch:\n",
    "            pred_ids = result.prediction  # prediction is directly the predict ids [pad is removed in decoder]\n",
    "            feature = result.feature\n",
    "            pred_label = set([id2label[pred_id] for pred_id in pred_ids])\n",
    "            label = set(feature.test_feature_item.data_item.label)\n",
    "            writing_content.append({\n",
    "                'seq_in': feature.test_feature_item.data_item.seq_in,\n",
    "                'pred': list(pred_label),\n",
    "                'label': list(label),\n",
    "            })\n",
    "            tp, fp, fn = self.update_f1_frag(pred_label, label, tp, fp, fn)  # update tp, fp, fn\n",
    "\n",
    "        with open(output_prediction_file, \"w\") as writer:\n",
    "            json.dump(writing_content, writer, indent=2)\n",
    "        return self.compute_f1(tp, fp, fn)\n",
    "\n",
    "    def writing_mlc_prediction(self, fs_batch: List[RawResult], output_prediction_file: str, id2label: dict):\n",
    "        tp, fp, fn = 0, 0, 0\n",
    "        writing_content = []\n",
    "        for result in fs_batch:\n",
    "            pred_ids = result.prediction  # prediction is directly the predict ids [pad is removed in decoder]\n",
    "            feature = result.feature\n",
    "            pred_label = set([id2label[pred_id] for pred_id in pred_ids])\n",
    "            label = set(feature.test_feature_item.data_item.label)\n",
    "            writing_content.append({\n",
    "                'seq_in': feature.test_feature_item.data_item.seq_in,\n",
    "                'pred': list(pred_label),\n",
    "                'label': list(label),\n",
    "            })\n",
    "            tp, fp, fn = self.update_f1_frag(pred_label, label, tp, fp, fn)  # update tp, fp, fn\n",
    "\n",
    "        with open(output_prediction_file, \"w\") as writer:\n",
    "            json.dump(writing_content, writer, indent=2)\n",
    "        return self.compute_f1(tp, fp, fn)\n",
    "\n",
    "    def update_f1_frag(self, pred_label, label, tp=0, fp=0, fn=0):\n",
    "        tp += len(pred_label & label)\n",
    "        fp += len(pred_label - label)\n",
    "        fn += len(label - pred_label)\n",
    "        return tp, fp, fn\n",
    "\n",
    "    def compute_f1(self, tp, fp, fn):\n",
    "        tp += 0.0000001  # to avoid zero division\n",
    "        fp += 0.0000001\n",
    "        fn += 0.0000001\n",
    "        precision = 1.0 * tp / (tp + fp)\n",
    "        recall = 1.0 * tp / (tp + fn)\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "        return precision, recall, f1\n",
    "\n",
    "    def writing_sl_prediction(self, fs_batch: List[RawResult], output_prediction_file: str, id2label: dict):\n",
    "        writing_content = []\n",
    "        for result in fs_batch:\n",
    "            prediction = result.prediction\n",
    "            feature = result.feature\n",
    "            pred_ids = prediction  # prediction is directly the predict ids\n",
    "            if len(pred_ids) != len(feature.test_feature_item.data_item.seq_in):\n",
    "                raise RuntimeError(\"Failed to align the pred_ids to texts: {},{} \\n{},{} \\n{},{}\".format(\n",
    "                    len(pred_ids), pred_ids,\n",
    "                    len(feature.test_feature_item.data_item.seq_in), feature.test_feature_item.data_item.seq_in,\n",
    "                    len(feature.test_feature_item.data_item.seq_out), feature.test_feature_item.data_item.seq_out\n",
    "                ))\n",
    "            for pred_id, word, true_label in zip(pred_ids, feature.test_feature_item.data_item.seq_in, feature.test_feature_item.data_item.seq_out):\n",
    "                pred_label = id2label[pred_id]\n",
    "                writing_content.append('{0} {1} {2}'.format(word, true_label, pred_label))\n",
    "            writing_content.append('')\n",
    "        with open(output_prediction_file, \"w\") as writer:\n",
    "            writer.write('\\n'.join(writing_content))\n",
    "\n",
    "    def eval_with_script(self, output_prediction_file):\n",
    "        script_args = ['perl', self.opt.eval_script]\n",
    "        with open(output_prediction_file, 'r') as res_file:\n",
    "            p = subprocess.Popen(script_args, stdout=subprocess.PIPE, stdin=res_file)\n",
    "            # logging.info('Eval script args:{0}'.format(p.args))\n",
    "            p.wait()\n",
    "\n",
    "            std_results = p.stdout.readlines()\n",
    "            if self.opt.verbose:\n",
    "                for r in std_results:\n",
    "                    print(r)\n",
    "            std_results = str(std_results[1]).split()\n",
    "        precision = float(std_results[3].replace('%;', ''))\n",
    "        recall = float(std_results[5].replace('%;', ''))\n",
    "        f1 = float(std_results[7].replace('%;', '').replace(\"\\\\n'\", ''))\n",
    "        return precision, recall, f1\n",
    "\n",
    "    def reform_few_shot_batch(self, all_results: List[RawResult]) -> List[List[Tuple[int, RawResult]]]:\n",
    "        \"\"\"\n",
    "        Our result score is average score of all few-shot batches.\n",
    "        So here, we classify all result according to few-shot batch id.\n",
    "        \"\"\"\n",
    "        \n",
    "        # 拿到few-shot batch id 到 result的映射\n",
    "        all_batches = {}\n",
    "        for result in all_results:\n",
    "            b_id = result.feature.batch_gid\n",
    "            if b_id not in all_batches:\n",
    "                all_batches[b_id] = [result]\n",
    "            else:\n",
    "                all_batches[b_id].append(result)\n",
    "        return sorted(all_batches.items(), key=lambda x: x[0])\n",
    "\n",
    "    def unpack_feature(self, feature: FewShotFeature) -> List[torch.Tensor]:\n",
    "        ret = [\n",
    "            torch.LongTensor([feature.gid]),\n",
    "            # test\n",
    "            feature.test_input.token_ids,\n",
    "            feature.test_input.segment_ids,\n",
    "            feature.test_input.nwp_index,\n",
    "            feature.test_input.input_mask,\n",
    "            feature.test_input.output_mask,\n",
    "            # support\n",
    "            feature.support_input.token_ids,\n",
    "            feature.support_input.segment_ids,\n",
    "            feature.support_input.nwp_index,\n",
    "            feature.support_input.input_mask,\n",
    "            feature.support_input.output_mask,\n",
    "            # target\n",
    "            feature.test_target,\n",
    "            feature.support_target,\n",
    "            # Special\n",
    "            torch.LongTensor([len(feature.support_feature_items)]),  # support num\n",
    "        ]\n",
    "        return ret\n",
    "\n",
    "    def do_forward(self, batch, model):\n",
    "        (\n",
    "            gid,  # 0\n",
    "            test_token_ids,  # 1\n",
    "            test_segment_ids,  # 2\n",
    "            test_nwp_index,  # 3\n",
    "            test_input_mask,  # 4\n",
    "            test_output_mask,  # 5\n",
    "            support_token_ids,  # 6\n",
    "            support_segment_ids,  # 7\n",
    "            support_nwp_index,  # 8\n",
    "            support_input_mask,  # 9\n",
    "            support_output_mask,  # 10\n",
    "            test_target,  # 11\n",
    "            support_target,  # 12\n",
    "            support_num,  # 13\n",
    "        ) = batch\n",
    "\n",
    "        prediction = model(\n",
    "            test_token_ids,\n",
    "            test_segment_ids,\n",
    "            test_nwp_index,\n",
    "            test_input_mask,\n",
    "            test_output_mask,\n",
    "            support_token_ids,\n",
    "            support_segment_ids,\n",
    "            support_nwp_index,\n",
    "            support_input_mask,\n",
    "            support_output_mask,\n",
    "            test_target,\n",
    "            support_target,\n",
    "            support_num,\n",
    "        )\n",
    "        return prediction\n",
    "\n",
    "    def get_value_from_order_dict(self, order_dict, key):\n",
    "        \"\"\"\"\"\"\n",
    "        for k, v in order_dict.items():\n",
    "            if key in k:\n",
    "                return v\n",
    "        return []\n",
    "\n",
    "    def clone_model(self, model, id2label):\n",
    "        \"\"\" clone only part of params \"\"\"\n",
    "        # deal with data parallel model\n",
    "        new_model: FewShotTextClassifier\n",
    "        old_model: FewShotTextClassifier\n",
    "        if self.opt.local_rank != -1 or self.n_gpu > 1 and hasattr(model, 'module'):  # the model is parallel class here\n",
    "            old_model = model.module\n",
    "        else:\n",
    "            old_model = model\n",
    "        emission_dict = old_model.emission_scorer.state_dict()\n",
    "        old_num_tags = len(self.get_value_from_order_dict(emission_dict, 'label_reps'))\n",
    "\n",
    "        config = {'num_tags': len(id2label), 'id2label': id2label}\n",
    "        if 'num_anchors' in old_model.config:\n",
    "            config['num_anchors'] = old_model.config['num_anchors']  # Use previous model's random anchors.\n",
    "        # get a new instance for different domain\n",
    "        new_model = make_model(opt=self.opt, config=config)\n",
    "        new_model = prepare_model(self.opt, new_model, self.device, self.n_gpu)\n",
    "        if self.opt.local_rank != -1 or self.n_gpu > 1:\n",
    "            sub_new_model = new_model.module\n",
    "        else:\n",
    "            sub_new_model = new_model\n",
    "        \n",
    "        ''' copy weights and stuff '''\n",
    "        if old_model.opt.task == 'sl' and old_model.transition_scorer:\n",
    "            # copy one-by-one because target transition and decoder will be left un-assigned\n",
    "            sub_new_model.context_embedder.load_state_dict(old_model.context_embedder.state_dict())\n",
    "            sub_new_model.emission_scorer.load_state_dict(old_model.emission_scorer.state_dict())\n",
    "            for param_name in ['backoff_trans_mat', 'backoff_start_trans_mat', 'backoff_end_trans_mat']:\n",
    "                sub_new_model.transition_scorer.state_dict()[param_name].copy_(\n",
    "                    old_model.transition_scorer.state_dict()[param_name].data)\n",
    "        else:\n",
    "            sub_new_model.load_state_dict(old_model.state_dict())\n",
    "        return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchemaFewShotTester(FewShotTester):\n",
    "    def __init__(self, opt, device, n_gpu):\n",
    "        super(SchemaFewShotTester, self).__init__(opt, device, n_gpu)\n",
    "\n",
    "    def get_data_loader(self, features):\n",
    "        \"\"\" add label index into special padding \"\"\"\n",
    "        dataset = FewShotDataset([self.unpack_feature(f) for f in features])\n",
    "        if self.opt.local_rank == -1:\n",
    "            sampler = SequentialSampler(dataset)\n",
    "        else:\n",
    "            sampler = DistributedSampler(dataset)\n",
    "        pad_collate = PadCollate(dim=-1, sp_dim=-2, sp_item_idx=[3, 8, 12, 16])  # nwp_index, spt_tgt need sp-padding\n",
    "        data_loader = DataLoader(dataset, sampler=sampler, batch_size=self.batch_size, collate_fn=pad_collate)\n",
    "        return data_loader\n",
    "\n",
    "    def unpack_feature(self, feature: FewShotFeature) -> List[torch.Tensor]:\n",
    "        ret = [\n",
    "            torch.LongTensor([feature.gid]),\n",
    "            # test\n",
    "            feature.test_input.token_ids,\n",
    "            feature.test_input.segment_ids,\n",
    "            feature.test_input.nwp_index,\n",
    "            feature.test_input.input_mask,\n",
    "            feature.test_input.output_mask,\n",
    "            # support\n",
    "            feature.support_input.token_ids,\n",
    "            feature.support_input.segment_ids,\n",
    "            feature.support_input.nwp_index,\n",
    "            feature.support_input.input_mask,\n",
    "            feature.support_input.output_mask,\n",
    "            # target\n",
    "            feature.test_target,\n",
    "            feature.support_target,\n",
    "            # Special\n",
    "            torch.LongTensor([len(feature.support_feature_items)]),  # support num\n",
    "            # label feature\n",
    "            feature.label_input.token_ids,\n",
    "            feature.label_input.segment_ids,\n",
    "            feature.label_input.nwp_index,\n",
    "            feature.label_input.input_mask,\n",
    "            feature.label_input.output_mask,\n",
    "        ]\n",
    "        return ret\n",
    "\n",
    "    def do_forward(self, batch, model):\n",
    "        (\n",
    "            gid,  # 0\n",
    "            test_token_ids,  # 1\n",
    "            test_segment_ids,  # 2\n",
    "            test_nwp_index,  # 3\n",
    "            test_input_mask,  # 4\n",
    "            test_output_mask,  # 5\n",
    "            support_token_ids,  # 6\n",
    "            support_segment_ids,  # 7\n",
    "            support_nwp_index,  # 8\n",
    "            support_input_mask,  # 9\n",
    "            support_output_mask,  # 10\n",
    "            test_target,  # 11\n",
    "            support_target,  # 12\n",
    "            support_num,  # 13\n",
    "            # label feature\n",
    "            label_token_ids,  # 14\n",
    "            label_segment_ids,  # 15\n",
    "            label_nwp_index,  # 16\n",
    "            label_input_mask,  # 17\n",
    "            label_output_mask,  # 18\n",
    "        ) = batch\n",
    "\n",
    "        prediction = model(\n",
    "            test_token_ids,\n",
    "            test_segment_ids,\n",
    "            test_nwp_index,\n",
    "            test_input_mask,\n",
    "            test_output_mask,\n",
    "            support_token_ids,\n",
    "            support_segment_ids,\n",
    "            support_nwp_index,\n",
    "            support_input_mask,\n",
    "            support_output_mask,\n",
    "            test_target,\n",
    "            support_target,\n",
    "            support_num,\n",
    "            # label feature\n",
    "            label_token_ids,\n",
    "            label_segment_ids,\n",
    "            label_nwp_index,\n",
    "            label_input_mask,\n",
    "            label_output_mask,\n",
    "        )\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelNumSchemaFewShotTester(SchemaFewShotTester):\n",
    "    def __init__(self, opt, device, n_gpu):\n",
    "        super(LabelNumSchemaFewShotTester, self).__init__(opt, device, n_gpu)\n",
    "\n",
    "    def get_data_loader(self, features):\n",
    "        \"\"\" add label index into special padding \"\"\"\n",
    "        dataset = FewShotDataset([self.unpack_feature(f) for f in features])\n",
    "        if self.opt.local_rank == -1:\n",
    "            sampler = SequentialSampler(dataset)\n",
    "        else:\n",
    "            sampler = DistributedSampler(dataset)\n",
    "        pad_collate = PadCollate(dim=-1, sp_dim=-2, sp_item_idx=[3, 8, 12, 16, 19])  # nwp_index, spt_tgt need sp-padding\n",
    "        data_loader = DataLoader(dataset, sampler=sampler, batch_size=self.batch_size, collate_fn=pad_collate)\n",
    "        return data_loader\n",
    "\n",
    "    def unpack_feature(self, feature: FewShotFeature) -> List[torch.Tensor]:\n",
    "        ret = [\n",
    "            torch.LongTensor([feature.gid]),\n",
    "            # test\n",
    "            feature.test_input.token_ids,\n",
    "            feature.test_input.segment_ids,\n",
    "            feature.test_input.nwp_index,\n",
    "            feature.test_input.input_mask,\n",
    "            feature.test_input.output_mask,\n",
    "            # support\n",
    "            feature.support_input.token_ids,\n",
    "            feature.support_input.segment_ids,\n",
    "            feature.support_input.nwp_index,\n",
    "            feature.support_input.input_mask,\n",
    "            feature.support_input.output_mask,\n",
    "            # target\n",
    "            feature.test_target,\n",
    "            feature.support_target,\n",
    "            # Special\n",
    "            torch.LongTensor([len(feature.support_feature_items)]),  # support num\n",
    "            # label feature\n",
    "            feature.label_input.token_ids,\n",
    "            feature.label_input.segment_ids,\n",
    "            feature.label_input.nwp_index,\n",
    "            feature.label_input.input_mask,\n",
    "            feature.label_input.output_mask,\n",
    "            # sentence feature\n",
    "            feature.support_label_num_feature,  # 19\n",
    "            feature.test_label_num_feature,  # 20\n",
    "            feature.support_label_num_target,  # 21\n",
    "            feature.test_label_num_target  # 22\n",
    "        ]\n",
    "        return ret\n",
    "\n",
    "    def do_forward(self, batch, model):\n",
    "        (\n",
    "            gid,  # 0\n",
    "            test_token_ids,  # 1\n",
    "            test_segment_ids,  # 2\n",
    "            test_nwp_index,  # 3\n",
    "            test_input_mask,  # 4\n",
    "            test_output_mask,  # 5\n",
    "            support_token_ids,  # 6\n",
    "            support_segment_ids,  # 7\n",
    "            support_nwp_index,  # 8\n",
    "            support_input_mask,  # 9\n",
    "            support_output_mask,  # 10\n",
    "            test_target,  # 11\n",
    "            support_target,  # 12\n",
    "            support_num,  # 13\n",
    "            # label feature\n",
    "            label_token_ids,  # 14\n",
    "            label_segment_ids,  # 15\n",
    "            label_nwp_index,  # 16\n",
    "            label_input_mask,  # 17\n",
    "            label_output_mask,  # 18\n",
    "            # sentence feature\n",
    "            support_label_num_feature,  # 19\n",
    "            test_label_num_feature,  # 20\n",
    "            support_label_num_target,  # 21\n",
    "            test_label_num_target  # 22\n",
    "        ) = batch\n",
    "\n",
    "        prediction = model(\n",
    "            test_token_ids,\n",
    "            test_segment_ids,\n",
    "            test_nwp_index,\n",
    "            test_input_mask,\n",
    "            test_output_mask,\n",
    "            support_token_ids,\n",
    "            support_segment_ids,\n",
    "            support_nwp_index,\n",
    "            support_input_mask,\n",
    "            support_output_mask,\n",
    "            test_target,\n",
    "            support_target,\n",
    "            support_num,\n",
    "            # label feature\n",
    "            label_token_ids,\n",
    "            label_segment_ids,\n",
    "            label_nwp_index,\n",
    "            label_input_mask,\n",
    "            label_output_mask,\n",
    "            # sentence feature\n",
    "            support_label_num_feature,  # 19\n",
    "            test_label_num_feature,  # 20\n",
    "            support_label_num_target,  # 21\n",
    "            test_label_num_target  # 22\n",
    "        )\n",
    "        return prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
