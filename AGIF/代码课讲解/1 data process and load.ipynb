{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理模块\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**目录：**\n",
    "1. 标签类别收集\n",
    "\n",
    "2. 单词词汇表收集\n",
    "\n",
    "3. 训练样本读取\n",
    "\n",
    "3. 样本转化为模型可读的特征\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aTsg5j62xfnk"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "from ordered_set import OrderedSet\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, RandomSampler, DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 训练数据形式回顾\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四个数据集\n",
    "\n",
    "<img src=\"./datasets.png\"  width=\"200\" height=\"300\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集格式\n",
    "\n",
    "<img src=\"./数据集格式.png\"  width=\"300\" height=\"300\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 词汇表定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedSet(['a', 'b', 'r', 'c', 'd'])\n",
      "True\n",
      "2\n",
      "r\n",
      "OrderedSet(['a', 'b', 'r', 'c', 'd', 'x'])\n"
     ]
    }
   ],
   "source": [
    "# orderedset\n",
    "#    是一种可变的数据结构，它是列表和集合的混合体。 \n",
    "#    它记住条目的顺序，\n",
    "#    每个条目都有一个索引号 可以查到。\n",
    "\n",
    "letters = OrderedSet('abracadabra')\n",
    "print(letters)\n",
    "print('r' in letters)\n",
    "print(letters.index('r'))\n",
    "print(letters[2])\n",
    "\n",
    "letters.add('x')\n",
    "print(letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Alphabet(object):\n",
    "    \"\"\"\n",
    "    Storage and serialization a set of elements.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, if_use_pad, if_use_unk):\n",
    "\n",
    "        self.__name = name\n",
    "        self.__if_use_pad = if_use_pad\n",
    "        self.__if_use_unk = if_use_unk\n",
    "\n",
    "        self.__index2instance = OrderedSet()  # orderset[3]\n",
    "        self.__instance2index = OrderedDict()  \n",
    "\n",
    "        # Counter Object record the frequency\n",
    "        # of element occurs in raw text.\n",
    "        self.__counter = Counter()\n",
    "\n",
    "        if if_use_pad:\n",
    "            self.__sign_pad = \"<PAD>\"\n",
    "            self.add_instance(self.__sign_pad)\n",
    "        if if_use_unk:\n",
    "            self.__sign_unk = \"<UNK>\"\n",
    "            self.add_instance(self.__sign_unk)\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.__name\n",
    "\n",
    "    def add_instance(self, instance, multi_intent=False):\n",
    "        \"\"\" Add instances to alphabet.\n",
    "\n",
    "        1, We support any iterative data structure which\n",
    "        contains elements of str type.  支持一次添加多个instances\n",
    "\n",
    "        2, We will count added instances that will influence\n",
    "        the serialization of unknown instance.\n",
    "\n",
    "        :param instance: is given instance or a list of it.\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(instance, (list, tuple)):\n",
    "            for element in instance:\n",
    "                self.add_instance(element, multi_intent=multi_intent)   # 递归\n",
    "            return\n",
    "\n",
    "        # We only support elements of str type.\n",
    "        assert isinstance(instance, str)\n",
    "        if multi_intent and '#' in instance:   # 针对多个意图标签的场景：e.g., RateBook#SearchScreeningEvent\n",
    "            for element in instance.split('#'):\n",
    "                self.add_instance(element, multi_intent=multi_intent)\n",
    "            return\n",
    "        \n",
    "        # count the frequency of instances.\n",
    "        self.__counter[instance] += 1\n",
    "\n",
    "        if instance not in self.__index2instance:\n",
    "            self.__instance2index[instance] = len(self.__index2instance)\n",
    "            self.__index2instance.append(instance)\n",
    "\n",
    "    def get_index(self, instance, multi_intent=False):\n",
    "        \"\"\" Serialize given instance and return.\n",
    "\n",
    "        For unknown words, the return index of alphabet\n",
    "        depends on variable self.__use_unk:\n",
    "\n",
    "            1, If True, then return the index of \"<UNK>\";\n",
    "            2, If False, then return the index of the\n",
    "            element that hold max frequency in training data.\n",
    "\n",
    "        :param instance: is given instance or a list of it.\n",
    "        :return: is the serialization of query instance.\n",
    "        \"\"\"\n",
    "        \n",
    "        # 使用递归的写法，支持 instance列表 取索引\n",
    "        if isinstance(instance, (list, tuple)):\n",
    "            return [self.get_index(elem, multi_intent=multi_intent) for elem in instance]\n",
    "\n",
    "        assert isinstance(instance, str)\n",
    "        if multi_intent and '#' in instance:  # 针对多个意图标签的场景\n",
    "            return [self.get_index(element, multi_intent=multi_intent) for element in instance.split('#')]\n",
    "\n",
    "        try:\n",
    "            return self.__instance2index[instance]\n",
    "        except KeyError:\n",
    "            if self.__if_use_unk:\n",
    "                return self.__instance2index[self.__sign_unk]\n",
    "            else:\n",
    "                max_freq_item = self.__counter.most_common(1)[0][0]   # counter.most_common(k): [(x1,freq1), (..., ,,,)]\n",
    "                return self.__instance2index[max_freq_item]\n",
    "\n",
    "    def get_instance(self, index):\n",
    "        \"\"\" Get corresponding instance of query index.\n",
    "\n",
    "        if index is invalid, then throws exception.\n",
    "\n",
    "        :param index: is query index, possibly iterable.\n",
    "        :return: is corresponding instance.\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(index, list):\n",
    "            return [self.get_instance(elem) for elem in index]\n",
    "\n",
    "        return self.__index2instance[index]\n",
    "\n",
    "    def save_content(self, dir_path):\n",
    "        \"\"\" Save the content of alphabet to files.\n",
    "\n",
    "        There are two kinds of saved files:\n",
    "            1, The first is a list file, elements are\n",
    "            sorted by the frequency of occurrence.  # 根据频率排序\n",
    "\n",
    "            2, The second is a dictionary file, elements\n",
    "            are sorted by it serialized index.      # 与其索引一起存入文件\n",
    "\n",
    "        :param dir_path: is the directory path to save object.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if dir_path exists.\n",
    "        if not os.path.exists(dir_path):\n",
    "            os.mkdir(dir_path)\n",
    "\n",
    "        list_path = os.path.join(dir_path, self.__name + \"_list.txt\")\n",
    "        with open(list_path, 'w', encoding=\"utf8\") as fw:\n",
    "            for element, frequency in self.__counter.most_common():\n",
    "                fw.write(element + '\\t' + str(frequency) + '\\n')\n",
    "\n",
    "        dict_path = os.path.join(dir_path, self.__name + \"_dict.txt\")\n",
    "        with open(dict_path, 'w', encoding=\"utf8\") as fw:\n",
    "            for index, element in enumerate(self.__index2instance):\n",
    "                fw.write(element + '\\t' + str(index) + '\\n')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.__index2instance)\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Alphabet {} contains about {} words: \\n\\t{}'.format(self.name, len(self), self.__index2instance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3 数据加载与处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetManager(object):\n",
    "\n",
    "    def __init__(self, args):\n",
    "\n",
    "        # Instantiate alphabet objects.\n",
    "        self.__word_alphabet = Alphabet('word', if_use_pad=True, if_use_unk=True)\n",
    "        self.__slot_alphabet = Alphabet('slot', if_use_pad=False, if_use_unk=False)\n",
    "        self.__intent_alphabet = Alphabet('intent', if_use_pad=False, if_use_unk=False)\n",
    "\n",
    "        # Record the raw text of dataset.\n",
    "        self.__text_word_data = {}\n",
    "        self.__text_slot_data = {}\n",
    "        self.__text_intent_data = {}\n",
    "\n",
    "        # Record the serialization of dataset.\n",
    "        self.__digit_word_data = {}\n",
    "        self.__digit_slot_data = {}\n",
    "        self.__digit_intent_data = {}\n",
    "\n",
    "        self.__args = args\n",
    "\n",
    "    @property    # 将类的方法变为属性；被property修饰的方法只有一个参数，self；必须有返回值；\n",
    "    def test_sentence(self):\n",
    "        return deepcopy(self.__text_word_data['test'])\n",
    "    \n",
    "    @property\n",
    "    def train_digit_word_data(self):\n",
    "        return deepcopy(self.__digit_word_data['train'])\n",
    "\n",
    "    @property\n",
    "    def word_alphabet(self):\n",
    "        return deepcopy(self.__word_alphabet)\n",
    "\n",
    "    @property\n",
    "    def slot_alphabet(self):\n",
    "        return deepcopy(self.__slot_alphabet)\n",
    "\n",
    "    @property\n",
    "    def intent_alphabet(self):\n",
    "        return deepcopy(self.__intent_alphabet)\n",
    "\n",
    "    @property\n",
    "    def num_epoch(self):\n",
    "        return self.__args.num_epoch\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return self.__args.batch_size\n",
    "\n",
    "    @property\n",
    "    def learning_rate(self):\n",
    "        return self.__args.learning_rate\n",
    "\n",
    "    @property\n",
    "    def l2_penalty(self):\n",
    "        return self.__args.l2_penalty\n",
    "\n",
    "    @property\n",
    "    def save_dir(self):\n",
    "        return self.__args.save_dir\n",
    "\n",
    "    @property\n",
    "    def slot_forcing_rate(self):\n",
    "        return self.__args.slot_forcing_rate\n",
    "    \n",
    "    # 读取数据\n",
    "    @staticmethod\n",
    "    def __read_file(file_path):\n",
    "        \"\"\" \n",
    "        Read data file of given path.\n",
    "\n",
    "        :param file_path: path of data file.\n",
    "        :return: list of sentence, list of slot and list of intent.\n",
    "        \"\"\"\n",
    "\n",
    "        texts, slots, intents = [], [], []\n",
    "        text, slot = [], []\n",
    "\n",
    "        with open(file_path, 'r', encoding=\"utf8\") as fr:\n",
    "            for line in fr.readlines():\n",
    "                items = line.strip().split()\n",
    "\n",
    "                if len(items) == 1:   # 表示：到了一个标签行\n",
    "                    texts.append(text)\n",
    "                    slots.append(slot)\n",
    "                    if \"/\" not in items[0]:\n",
    "                        intents.append(items)\n",
    "                    else:\n",
    "                        print(items)\n",
    "                        new = items[0].split(\"/\")\n",
    "                        intents.append([new[1]])\n",
    "\n",
    "                    # clear buffer lists.\n",
    "                    text, slot = [], []\n",
    "\n",
    "                elif len(items) == 2:\n",
    "                    text.append(items[0].strip())\n",
    "                    slot.append(items[1].strip())\n",
    "\n",
    "        return texts, slots, intents\n",
    "    \n",
    "    \n",
    "    def add_file(self, file_path, data_name, if_train_file):\n",
    "        text, slot, intent = self.__read_file(file_path)\n",
    "\n",
    "        if if_train_file:\n",
    "            self.__word_alphabet.add_instance(text)\n",
    "            self.__slot_alphabet.add_instance(slot)\n",
    "            self.__intent_alphabet.add_instance(intent, multi_intent=True)\n",
    "\n",
    "        # Record the raw text of dataset.\n",
    "        self.__text_word_data[data_name] = text\n",
    "        self.__text_slot_data[data_name] = slot\n",
    "        self.__text_intent_data[data_name] = intent\n",
    "\n",
    "        # Serialize raw text and stored it.\n",
    "        self.__digit_word_data[data_name] = self.__word_alphabet.get_index(text)\n",
    "        if if_train_file:\n",
    "            self.__digit_slot_data[data_name] = self.__slot_alphabet.get_index(slot)\n",
    "            self.__digit_intent_data[data_name] = self.__intent_alphabet.get_index(intent, multi_intent=True)\n",
    "    \n",
    "    def quick_build(self):\n",
    "        \"\"\"\n",
    "        Convenient function to instantiate a dataset object.\n",
    "        \"\"\"\n",
    "\n",
    "        train_path = os.path.join(self.__args.data_dir, 'train.txt')\n",
    "        dev_path = os.path.join(self.__args.data_dir, 'dev.txt')\n",
    "        test_path = os.path.join(self.__args.data_dir, 'test.txt')\n",
    "        \n",
    "        # add_file: 读入数据，并做初步处理\n",
    "        self.add_file(train_path, 'train', if_train_file=True)   \n",
    "        self.add_file(dev_path, 'dev', if_train_file=False)\n",
    "        self.add_file(test_path, 'test', if_train_file=False)\n",
    "\n",
    "        # Check if save path exists.\n",
    "        if not os.path.exists(self.save_dir):\n",
    "            os.mkdir(self.save_dir)\n",
    "\n",
    "        alphabet_dir = os.path.join(self.__args.save_dir, \"alphabet\")\n",
    "        self.__word_alphabet.save_content(alphabet_dir)\n",
    "        self.__slot_alphabet.save_content(alphabet_dir)\n",
    "        self.__intent_alphabet.save_content(alphabet_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "828\n",
      "13162\n",
      "[39, 95, 42, 109, 110, 5, 11, 12, 92, 62, 53, 86, 56, 79, 27, 111, 66, 88, 112, 68, 50, 113, 9, 102, 44, 45, 114, 95, 9, 115, 24, 64]\n"
     ]
    }
   ],
   "source": [
    "# 先构建参数\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "\n",
    "# 实际使用应该是命令行传入的参数，不过我这里直接赋值传入\n",
    "# parser.add_argument(\"--task\", default=None, required=True, type=str, help=\"The name of the task to train\")\n",
    "# parser.add_argument(\"--data_dir\", default=\"./data\", type=str, help=\"The input data dir\")\n",
    "# parser.add_argument(\"--intent_label_file\", default=\"intent_label.txt\", type=str, help=\"Intent Label file\")\n",
    "# parser.add_argument(\"--slot_label_file\", default=\"slot_label.txt\", type=str, help=\"Slot Label file\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "class Args():\n",
    "    task =  None\n",
    "    \n",
    "\n",
    "args = Args()\n",
    "args.data_dir = \"../data/MixATIS_clean\"\n",
    "args.save_dir = \"../save/MixATIS_clean\"\n",
    "\n",
    "# Instantiate a dataset object.\n",
    "dataset = DatasetManager(args)\n",
    "dataset.quick_build()\n",
    "# dataset.show_summary()\n",
    "\n",
    "print(len(dataset.test_sentence))\n",
    "print(len(dataset.train_digit_word_data))\n",
    "print(dataset.train_digit_word_data[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 转化为特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetManager(object):\n",
    "\n",
    "    def __init__(self, args):\n",
    "\n",
    "        # Instantiate alphabet objects.\n",
    "        self.__word_alphabet = Alphabet('word', if_use_pad=True, if_use_unk=True)\n",
    "        self.__slot_alphabet = Alphabet('slot', if_use_pad=False, if_use_unk=False)\n",
    "        self.__intent_alphabet = Alphabet('intent', if_use_pad=False, if_use_unk=False)\n",
    "\n",
    "        # Record the raw text of dataset.\n",
    "        self.__text_word_data = {}\n",
    "        self.__text_slot_data = {}\n",
    "        self.__text_intent_data = {}\n",
    "\n",
    "        # Record the serialization of dataset.\n",
    "        self.__digit_word_data = {}\n",
    "        self.__digit_slot_data = {}\n",
    "        self.__digit_intent_data = {}\n",
    "\n",
    "        self.__args = args\n",
    "\n",
    "    @property    # 将类的方法变为属性；被property修饰的方法只有一个参数，self；必须有返回值；\n",
    "    def test_sentence(self):\n",
    "        return deepcopy(self.__text_word_data['test'])\n",
    "    \n",
    "    @property\n",
    "    def train_digit_word_data(self):\n",
    "        return deepcopy(self.__digit_word_data['train'])\n",
    "\n",
    "    @property\n",
    "    def word_alphabet(self):\n",
    "        return deepcopy(self.__word_alphabet)\n",
    "\n",
    "    @property\n",
    "    def slot_alphabet(self):\n",
    "        return deepcopy(self.__slot_alphabet)\n",
    "\n",
    "    @property\n",
    "    def intent_alphabet(self):\n",
    "        return deepcopy(self.__intent_alphabet)\n",
    "\n",
    "    @property\n",
    "    def num_epoch(self):\n",
    "        return self.__args.num_epoch\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return self.__args.batch_size\n",
    "\n",
    "    @property\n",
    "    def learning_rate(self):\n",
    "        return self.__args.learning_rate\n",
    "\n",
    "    @property\n",
    "    def l2_penalty(self):\n",
    "        return self.__args.l2_penalty\n",
    "\n",
    "    @property\n",
    "    def save_dir(self):\n",
    "        return self.__args.save_dir\n",
    "\n",
    "    @property\n",
    "    def slot_forcing_rate(self):\n",
    "        return self.__args.slot_forcing_rate\n",
    "    \n",
    "    # 读取数据\n",
    "    @staticmethod\n",
    "    def __read_file(file_path):\n",
    "        \"\"\" \n",
    "        Read data file of given path.\n",
    "\n",
    "        :param file_path: path of data file.\n",
    "        :return: list of sentence, list of slot and list of intent.\n",
    "        \"\"\"\n",
    "\n",
    "        texts, slots, intents = [], [], []\n",
    "        text, slot = [], []\n",
    "\n",
    "        with open(file_path, 'r', encoding=\"utf8\") as fr:\n",
    "            for line in fr.readlines():\n",
    "                items = line.strip().split()\n",
    "\n",
    "                if len(items) == 1:   # 表示：到了一个标签行\n",
    "                    texts.append(text)\n",
    "                    slots.append(slot)\n",
    "                    if \"/\" not in items[0]:\n",
    "                        intents.append(items)\n",
    "                    else:\n",
    "                        print(items)\n",
    "                        new = items[0].split(\"/\")\n",
    "                        intents.append([new[1]])\n",
    "\n",
    "                    # clear buffer lists.\n",
    "                    text, slot = [], []\n",
    "\n",
    "                elif len(items) == 2:\n",
    "                    text.append(items[0].strip())\n",
    "                    slot.append(items[1].strip())\n",
    "\n",
    "        return texts, slots, intents\n",
    "    \n",
    "    \n",
    "    def add_file(self, file_path, data_name, if_train_file):\n",
    "        text, slot, intent = self.__read_file(file_path)\n",
    "\n",
    "        if if_train_file:\n",
    "            self.__word_alphabet.add_instance(text)\n",
    "            self.__slot_alphabet.add_instance(slot)\n",
    "            self.__intent_alphabet.add_instance(intent, multi_intent=True)\n",
    "\n",
    "        # Record the raw text of dataset.\n",
    "        self.__text_word_data[data_name] = text\n",
    "        self.__text_slot_data[data_name] = slot\n",
    "        self.__text_intent_data[data_name] = intent\n",
    "\n",
    "        # Serialize raw text and stored it.\n",
    "        self.__digit_word_data[data_name] = self.__word_alphabet.get_index(text)\n",
    "        if if_train_file:\n",
    "            self.__digit_slot_data[data_name] = self.__slot_alphabet.get_index(slot)\n",
    "            self.__digit_intent_data[data_name] = self.__intent_alphabet.get_index(intent, multi_intent=True)\n",
    "    \n",
    "    def quick_build(self):\n",
    "        \"\"\"\n",
    "        Convenient function to instantiate a dataset object.\n",
    "        \"\"\"\n",
    "\n",
    "        train_path = os.path.join(self.__args.data_dir, 'train.txt')\n",
    "        dev_path = os.path.join(self.__args.data_dir, 'dev.txt')\n",
    "        test_path = os.path.join(self.__args.data_dir, 'test.txt')\n",
    "        \n",
    "        # add_file: 读入数据，并做初步处理\n",
    "        self.add_file(train_path, 'train', if_train_file=True)   \n",
    "        self.add_file(dev_path, 'dev', if_train_file=False)\n",
    "        self.add_file(test_path, 'test', if_train_file=False)\n",
    "\n",
    "        # Check if save path exists.\n",
    "        if not os.path.exists(self.save_dir):\n",
    "            os.mkdir(self.save_dir)\n",
    "\n",
    "        alphabet_dir = os.path.join(self.__args.save_dir, \"alphabet\")\n",
    "        self.__word_alphabet.save_content(alphabet_dir)\n",
    "        self.__slot_alphabet.save_content(alphabet_dir)\n",
    "        self.__intent_alphabet.save_content(alphabet_dir)\n",
    "    \n",
    "    def batch_delivery(self, data_name, batch_size=None, is_digital=True, shuffle=True):\n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "\n",
    "        if is_digital:\n",
    "            text = self.__digit_word_data[data_name]\n",
    "            slot = self.__digit_slot_data[data_name]\n",
    "            intent = self.__digit_intent_data[data_name]\n",
    "        else:\n",
    "            text = self.__text_word_data[data_name]\n",
    "            slot = self.__text_slot_data[data_name]\n",
    "            intent = self.__text_intent_data[data_name]\n",
    "        dataset = TorchDataset(text, slot, intent)\n",
    "\n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=self.__collate_fn)\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_padding(texts, items=None, digital=True):\n",
    "        len_list = [len(text) for text in texts]\n",
    "        max_len = max(len_list)\n",
    "\n",
    "        # Get sorted index of len_list.\n",
    "        sorted_index = np.argsort(len_list)[::-1]  # 按照长度倒序排列\n",
    "\n",
    "        trans_texts, seq_lens, trans_items = [], [], None\n",
    "        if items is not None:\n",
    "            trans_items = [[] for _ in range(0, len(items))]\n",
    "\n",
    "        for index in sorted_index:\n",
    "            seq_lens.append(deepcopy(len_list[index]))\n",
    "            trans_texts.append(deepcopy(texts[index]))\n",
    "            if digital:\n",
    "                trans_texts[-1].extend([0] * (max_len - len_list[index]))\n",
    "            else:\n",
    "                trans_texts[-1].extend(['<PAD>'] * (max_len - len_list[index]))\n",
    "\n",
    "            # This required specific if padding after sorting.\n",
    "            if items is not None:\n",
    "                for item, (o_item, required) in zip(trans_items, items):\n",
    "                    item.append(deepcopy(o_item[index]))\n",
    "                    if required:\n",
    "                        if digital:\n",
    "                            item[-1].extend([0] * (max_len - len_list[index]))\n",
    "                        else:\n",
    "                            item[-1].extend(['<PAD>'] * (max_len - len_list[index]))\n",
    "\n",
    "        if items is not None:\n",
    "            return trans_texts, trans_items, seq_lens\n",
    "        else:\n",
    "            return trans_texts, seq_lens\n",
    "    \n",
    "    @staticmethod\n",
    "    def __collate_fn(batch):\n",
    "        \"\"\"\n",
    "        helper function to instantiate a DataLoader Object.\n",
    "        \"\"\"\n",
    "\n",
    "        n_entity = len(batch[0])\n",
    "        modified_batch = [[] for _ in range(0, n_entity)]\n",
    "\n",
    "        for idx in range(0, len(batch)):\n",
    "            for jdx in range(0, n_entity):\n",
    "                modified_batch[jdx].append(batch[idx][jdx])\n",
    "\n",
    "        return modified_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMaJPtlMHur6DonwEyZLw5h",
   "collapsed_sections": [],
   "name": "data process and load.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
