{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**目录：**\n",
    "1. Trainer\n",
    "    - training：训练过程;\n",
    "        - 是否有teacher forcing\n",
    "        \n",
    "    - 评估；\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\兼职\\深度之眼\\论文讲堂\\02-AGIF\\代码复现\\AGIF\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "%cd ../\n",
    "# Utils functions copied from Slot-gated model, origin url:\n",
    "# \thttps://github.com/MiuLab/SlotGated-SLU/blob/master/utils.py\n",
    "from utils import miulab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilabel2one_hot(labels, nums):\n",
    "    res = [0.] * nums\n",
    "    if len(labels) == 0:\n",
    "        return res\n",
    "    if isinstance(labels[0], list):\n",
    "        for label in labels[0]:\n",
    "            res[label] = 1.\n",
    "        return res\n",
    "    for label in labels:\n",
    "        res[label] = 1.\n",
    "    return res\n",
    "\n",
    "\n",
    "def instance2onehot(func, num_intent, data):\n",
    "    res = []\n",
    "    for intents in func(data):\n",
    "        res.append(multilabel2one_hot(intents, num_intent))\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adj(mx):\n",
    "    \"\"\"\n",
    "    Row-normalize matrix  D^{-1}A 其中 D 是 度矩阵\n",
    "    torch.diag_embed: https://github.com/pytorch/pytorch/pull/12447\n",
    "    \"\"\"\n",
    "    mx = mx.float()\n",
    "    rowsum = mx.sum(2)\n",
    "    r_inv = torch.pow(rowsum, -1)\n",
    "    r_inv[torch.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = torch.diag_embed(r_inv, 0)\n",
    "    mx = r_mat_inv.matmul(mx)\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Processor(object):\n",
    "\n",
    "    def __init__(self, dataset, model, args):\n",
    "        self.__dataset = dataset\n",
    "        self.__model = model\n",
    "        self.args = args\n",
    "        self.__batch_size = args.batch_size\n",
    "        self.__load_dir = args.load_dir\n",
    "\n",
    "        if args.gpu:\n",
    "            time_start = time.time()\n",
    "            self.__model = self.__model.cuda()\n",
    "\n",
    "            time_con = time.time() - time_start\n",
    "            print(\"The model has been loaded into GPU and cost {:.6f} seconds.\\n\".format(time_con))\n",
    "\n",
    "        self.__criterion = nn.NLLLoss()\n",
    "        self.__criterion_intent = nn.BCEWithLogitsLoss()\n",
    "        self.__optimizer = optim.Adam(\n",
    "            self.__model.parameters(), lr=self.__dataset.learning_rate, weight_decay=self.__dataset.l2_penalty\n",
    "        )\n",
    "\n",
    "        if self.__load_dir:\n",
    "            if self.args.gpu:\n",
    "                print(\"MODEL {} LOADED\".format(str(self.__load_dir)))\n",
    "                self.__model = torch.load(os.path.join(self.__load_dir, 'model/model.pkl'))\n",
    "            else:\n",
    "                print(\"MODEL {} LOADED\".format(str(self.__load_dir)))\n",
    "                self.__model = torch.load(os.path.join(self.__load_dir, 'model/model.pkl'),\n",
    "                                          map_location=torch.device('cpu'))\n",
    "\n",
    "    def train(self):\n",
    "        best_dev_sent = 0.0\n",
    "        best_epoch = 0\n",
    "        no_improve = 0\n",
    "        dataloader = self.__dataset.batch_delivery('train')\n",
    "        for epoch in range(0, self.__dataset.num_epoch):\n",
    "            total_slot_loss, total_intent_loss = 0.0, 0.0\n",
    "            time_start = time.time()\n",
    "            self.__model.train()\n",
    "\n",
    "            for text_batch, slot_batch, intent_batch in tqdm(dataloader, ncols=50):\n",
    "                padded_text, [sorted_slot, sorted_intent], seq_lens = self.__dataset.add_padding(\n",
    "                    text_batch, [(slot_batch, True), (intent_batch, False)]\n",
    "                )\n",
    "                sorted_intent = [multilabel2one_hot(intents, len(self.__dataset.intent_alphabet)) for intents in\n",
    "                                 sorted_intent]\n",
    "                text_var = torch.LongTensor(padded_text)\n",
    "                slot_var = torch.LongTensor(sorted_slot)\n",
    "                intent_var = torch.Tensor(sorted_intent)\n",
    "                max_len = np.max(seq_lens)\n",
    "\n",
    "                if self.args.gpu:\n",
    "                    text_var = text_var.cuda()\n",
    "                    slot_var = slot_var.cuda()\n",
    "                    intent_var = intent_var.cuda()\n",
    "\n",
    "                random_slot, random_intent = random.random(), random.random()\n",
    "                \n",
    "                # 训练时，结合 teacher forcing 和 非teacher forcing，即真正的端到端\n",
    "                if random_slot < self.__dataset.slot_forcing_rate:\n",
    "                    slot_out, intent_out = self.__model(text_var, seq_lens, forced_slot=slot_var)\n",
    "                else:\n",
    "                    slot_out, intent_out = self.__model(text_var, seq_lens)\n",
    "\n",
    "                slot_var = torch.cat([slot_var[i][:seq_lens[i]] for i in range(0, len(seq_lens))], dim=0)\n",
    "                slot_loss = self.__criterion(slot_out, slot_var)\n",
    "                intent_loss = self.__criterion_intent(intent_out, intent_var)\n",
    "                batch_loss = slot_loss + intent_loss\n",
    "\n",
    "                self.__optimizer.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                self.__optimizer.step()\n",
    "\n",
    "                try:\n",
    "                    total_slot_loss += slot_loss.cpu().item()\n",
    "                    total_intent_loss += intent_loss.cpu().item()\n",
    "                except AttributeError:\n",
    "                    total_slot_loss += slot_loss.cpu().data.numpy()[0]\n",
    "                    total_intent_loss += intent_loss.cpu().data.numpy()[0]\n",
    "\n",
    "            time_con = time.time() - time_start\n",
    "            print(\n",
    "                '[Epoch {:2d}]: The total slot loss on train data is {:2.6f}, intent data is {:2.6f}, cost ' \\\n",
    "                'about {:2.6} seconds.'.format(epoch, total_slot_loss, total_intent_loss, time_con))\n",
    "            \n",
    "            # 先做一次dev上面的评估\n",
    "            change, time_start = False, time.time()\n",
    "            dev_slot_f1_score, dev_intent_f1_score, dev_intent_acc_score, dev_sent_acc_score = self.estimate(\n",
    "                if_dev=True,\n",
    "                test_batch=self.__batch_size,\n",
    "                args=self.args\n",
    "            )\n",
    "            \n",
    "            # 如果dev上面有进步，则评估一下测试结果\n",
    "            if dev_sent_acc_score > best_dev_sent:\n",
    "                no_improve = 0\n",
    "                best_epoch = epoch\n",
    "                best_dev_sent = dev_sent_acc_score\n",
    "                test_slot_f1, test_intent_f1, test_intent_acc, test_sent_acc = self.estimate(\n",
    "                    if_dev=False, test_batch=self.__batch_size, args=self.args)\n",
    "\n",
    "                print('\\nTest result: epoch: {}, slot f1 score: {:.6f}, intent f1 score: {:.6f}, intent acc score:'\n",
    "                      ' {:.6f}, semantic accuracy score: {:.6f}.'.\n",
    "                      format(epoch, test_slot_f1, test_intent_f1, test_intent_acc, test_sent_acc))\n",
    "\n",
    "                model_save_dir = os.path.join(self.__dataset.save_dir, \"model\")\n",
    "                if not os.path.exists(model_save_dir):\n",
    "                    os.mkdir(model_save_dir)\n",
    "\n",
    "                torch.save(self.__model, os.path.join(model_save_dir, \"model.pkl\"))\n",
    "                torch.save(self.__dataset, os.path.join(model_save_dir, 'dataset.pkl'))\n",
    "\n",
    "                time_con = time.time() - time_start\n",
    "                print('[Epoch {:2d}]: In validation process, the slot f1 score is {:2.6f}, ' \\\n",
    "                      'the intent f1 score is {:2.6f}, the intent acc score is {:2.6f}, the semantic acc is {:.2f}, cost about {:2.6f} seconds.\\n'.format(\n",
    "                    epoch, dev_slot_f1_score, dev_intent_f1_score, dev_intent_acc_score,\n",
    "                    dev_sent_acc_score, time_con))\n",
    "            else:\n",
    "                no_improve += 1\n",
    "\n",
    "            if self.args.early_stop == True:\n",
    "                if no_improve > self.args.patience:\n",
    "                    print('early stop at epoch {}'.format(epoch))\n",
    "                    break\n",
    "        print('Best epoch is {}'.format(best_epoch))\n",
    "        return best_epoch\n",
    "\n",
    "    def estimate(self, if_dev, args, test_batch=100):\n",
    "        \"\"\"\n",
    "        Estimate the performance of model on dev or test dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        if if_dev:\n",
    "            ss, pred_slot, real_slot, pred_intent, real_intent = self.prediction(\n",
    "                self.__model, self.__dataset, \"dev\", test_batch, args)\n",
    "        else:\n",
    "            ss, pred_slot, real_slot, pred_intent, real_intent = self.prediction(\n",
    "                self.__model, self.__dataset, \"test\", test_batch, args)\n",
    "\n",
    "        num_intent = len(self.__dataset.intent_alphabet)\n",
    "        slot_f1_score = miulab.computeF1Score(ss, real_slot, pred_slot, args)[0]\n",
    "        intent_f1_score = f1_score(\n",
    "            instance2onehot(self.__dataset.intent_alphabet.get_index, num_intent, real_intent),\n",
    "            instance2onehot(self.__dataset.intent_alphabet.get_index, num_intent, pred_intent),\n",
    "            average='macro')\n",
    "        intent_acc_score = Evaluator.intent_acc(pred_intent, real_intent)\n",
    "        sent_acc = Evaluator.semantic_acc(pred_slot, real_slot, pred_intent, real_intent)\n",
    "        print(\"slot f1: {}, intent f1: {}, intent acc: {}, exact acc: {}\".format(slot_f1_score, intent_f1_score,\n",
    "                                                                                 intent_acc_score, sent_acc))\n",
    "        # Write those sample both have intent and slot errors.\n",
    "        with open(os.path.join(args.save_dir, 'error.txt'), 'w', encoding=\"utf8\") as fw:\n",
    "            for p_slot_list, r_slot_list, p_intent_list, r_intent in \\\n",
    "                    zip(pred_slot, real_slot, pred_intent, real_intent):\n",
    "                fw.write(','.join(p_intent_list) + '\\t' + ','.join(r_intent) + '\\n')\n",
    "                for w, r_slot, in zip(p_slot_list, r_slot_list):\n",
    "                    fw.write(w + '\\t' + r_slot + '\\t''\\n')\n",
    "                fw.write('\\n\\n')\n",
    "\n",
    "        return slot_f1_score, intent_f1_score, intent_acc_score, sent_acc\n",
    "\n",
    "    @staticmethod\n",
    "    def validate(model_path, dataset, batch_size, num_intent, args):\n",
    "        \"\"\"\n",
    "        validation will write mistaken samples to files and make scores.\n",
    "        \"\"\"\n",
    "\n",
    "        if args.gpu:\n",
    "            model = torch.load(model_path)\n",
    "        else:\n",
    "            model = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "\n",
    "        ss, pred_slot, real_slot, pred_intent, real_intent = Processor.prediction(\n",
    "            model, dataset, \"test\", batch_size, args)\n",
    "\n",
    "        # To make sure the directory for save error prediction.\n",
    "        mistake_dir = os.path.join(dataset.save_dir, \"error\")\n",
    "        if not os.path.exists(mistake_dir):\n",
    "            os.mkdir(mistake_dir)\n",
    "\n",
    "        slot_f1_score = miulab.computeF1Score(ss, real_slot, pred_slot, args)[0]\n",
    "        intent_f1_score = f1_score(instance2onehot(dataset.intent_alphabet.get_index, num_intent, real_intent),\n",
    "                                   instance2onehot(dataset.intent_alphabet.get_index, num_intent, pred_intent),\n",
    "                                   average='macro')\n",
    "        intent_acc_score = Evaluator.intent_acc(pred_intent, real_intent)\n",
    "        sent_acc = Evaluator.semantic_acc(pred_slot, real_slot, pred_intent, real_intent)\n",
    "        print(\"slot f1: {}, intent f1: {}, intent acc: {}, exact acc: {}\".format(slot_f1_score, intent_f1_score,\n",
    "                                                                                 intent_acc_score, sent_acc))\n",
    "        # Write those sample both have intent and slot errors.\n",
    "\n",
    "        with open(os.path.join(args.save_dir, 'error.txt'), 'w', encoding=\"utf8\") as fw:\n",
    "            for p_slot_list, r_slot_list, p_intent_list, r_intent in \\\n",
    "                    zip(pred_slot, real_slot, pred_intent, real_intent):\n",
    "                fw.write(','.join(p_intent_list) + '\\t' + ','.join(r_intent) + '\\n')\n",
    "                for w, r_slot, in zip(p_slot_list, r_slot_list):\n",
    "                    fw.write(w + '\\t' + r_slot + '\\t''\\n')\n",
    "                fw.write('\\n\\n')\n",
    "        # with open(os.path.join(args.save_dir, 'slot_right.txt'), 'w', encoding=\"utf8\") as fw:\n",
    "        #     for p_slot_list, r_slot_list, tokens in \\\n",
    "        #             zip(pred_slot, real_slot, ss):\n",
    "        #         if p_slot_list != r_slot_list:\n",
    "        #             continue\n",
    "        #         fw.write(' '.join(tokens) + '\\n' + ' '.join(r_slot_list) + '\\n' + ' '.join(p_slot_list) + '\\n' + '\\n\\n')\n",
    "\n",
    "        return slot_f1_score, intent_f1_score, intent_acc_score, sent_acc\n",
    "\n",
    "    @staticmethod\n",
    "    def prediction(model, dataset, mode, batch_size, args):\n",
    "        model.eval()\n",
    "\n",
    "        if mode == \"dev\":\n",
    "            dataloader = dataset.batch_delivery('dev', batch_size=batch_size, shuffle=False, is_digital=False)\n",
    "        elif mode == \"test\":\n",
    "            dataloader = dataset.batch_delivery('test', batch_size=batch_size, shuffle=False, is_digital=False)\n",
    "        else:\n",
    "            raise Exception(\"Argument error! mode belongs to {\\\"dev\\\", \\\"test\\\"}.\")\n",
    "\n",
    "        pred_slot, real_slot = [], []\n",
    "        pred_intent, real_intent = [], []\n",
    "        all_token = []\n",
    "        for text_batch, slot_batch, intent_batch in tqdm(dataloader, ncols=50):\n",
    "            padded_text, [sorted_slot, sorted_intent], seq_lens = dataset.add_padding(\n",
    "                text_batch, [(slot_batch, False), (intent_batch, False)],\n",
    "                digital=False\n",
    "            )\n",
    "            real_slot.extend(sorted_slot)\n",
    "            all_token.extend([pt[:seq_lens[idx]] for idx, pt in enumerate(padded_text)])\n",
    "            for intents in list(Evaluator.expand_list(sorted_intent)):\n",
    "                if '#' in intents:\n",
    "                    real_intent.append(intents.split('#'))\n",
    "                else:\n",
    "                    real_intent.append([intents])\n",
    "\n",
    "            digit_text = dataset.word_alphabet.get_index(padded_text)\n",
    "            var_text = torch.LongTensor(digit_text)\n",
    "            max_len = np.max(seq_lens)\n",
    "            if args.gpu:\n",
    "                var_text = var_text.cuda()\n",
    "            slot_idx, intent_idx = model(var_text, seq_lens, n_predicts=1)\n",
    "            nested_slot = Evaluator.nested_list([list(Evaluator.expand_list(slot_idx))], seq_lens)[0]\n",
    "            pred_slot.extend(dataset.slot_alphabet.get_instance(nested_slot))\n",
    "            intent_idx_ = [[] for i in range(len(digit_text))]\n",
    "            for item in intent_idx:\n",
    "                intent_idx_[item[0]].append(item[1])\n",
    "            intent_idx = intent_idx_\n",
    "            pred_intent.extend(dataset.intent_alphabet.get_instance(intent_idx))\n",
    "        if 'MixSNIPS' in args.data_dir or 'MixATIS' in args.data_dir or 'DSTC' in args.data_dir:\n",
    "            [p_intent.sort() for p_intent in pred_intent]\n",
    "        with open(os.path.join(args.save_dir, 'token.txt'), \"w\", encoding=\"utf8\") as writer:\n",
    "            idx = 0\n",
    "            for line, slots, rss in zip(all_token, pred_slot, real_slot):\n",
    "                for c, sl, rsl in zip(line, slots, rss):\n",
    "                    writer.writelines(\n",
    "                        str(sl == rsl) + \" \" + c + \" \" + sl + \" \" + rsl + \"\\n\")\n",
    "                idx = idx + len(line)\n",
    "                writer.writelines(\"\\n\")\n",
    "\n",
    "        return all_token, pred_slot, real_slot, pred_intent, real_intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
