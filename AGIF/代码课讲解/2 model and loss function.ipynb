{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型构建与损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**目录：**\n",
    "1. AGIF模型\n",
    "    - GAT；\n",
    "    - 自注意力编码器；\n",
    "    - LSTM解码器\n",
    "\n",
    "\n",
    "2. 损失函数计算\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import TensorDataset, RandomSampler, DataLoader\n",
    "\n",
    "# 以BERT为预训练模型进行讲解\n",
    "from transformers import BertPreTrainedModel, BertModel, BertConfig\n",
    "from torchcrf import CRF  # pip install pytorch-crf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码器中 的 LSTM 模块\n",
    "\n",
    "class LSTMEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder structure based on bidirectional LSTM.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, dropout_rate):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "\n",
    "        # Parameter recording.\n",
    "        self.__embedding_dim = embedding_dim\n",
    "        self.__hidden_dim = hidden_dim // 2\n",
    "        self.__dropout_rate = dropout_rate\n",
    "\n",
    "        # Network attributes.\n",
    "        self.__dropout_layer = nn.Dropout(self.__dropout_rate)\n",
    "        \n",
    "        self.__lstm_layer = nn.LSTM(\n",
    "            input_size=self.__embedding_dim,\n",
    "            hidden_size=self.__hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=self.__dropout_rate,\n",
    "            num_layers=1\n",
    "        )\n",
    "\n",
    "    def forward(self, embedded_text, seq_lens):\n",
    "        \"\"\" Forward process for LSTM Encoder.\n",
    "\n",
    "        (batch_size, max_sent_len)\n",
    "        -> (batch_size, max_sent_len, word_dim)\n",
    "        -> (batch_size, max_sent_len, hidden_dim)\n",
    "        -> (total_word_num, hidden_dim)\n",
    "\n",
    "        :param embedded_text: padded and embedded input text.\n",
    "        :param seq_lens: is the length of original input text.\n",
    "        :return: is encoded word hidden vectors.\n",
    "        \"\"\"\n",
    "\n",
    "        # Padded_text should be instance of LongTensor. \n",
    "        dropout_text = self.__dropout_layer(embedded_text)\n",
    "\n",
    "        # Pack and Pad process for input of variable length.  \n",
    "        # 让LSTM知道句子哪些地方是padding，哪些不是；\n",
    "        # 这样LSTM就不会编码padding的部分，结果会更精确\n",
    "        packed_text = pack_padded_sequence(dropout_text, seq_lens, batch_first=True)\n",
    "        lstm_hiddens, (h_last, c_last) = self.__lstm_layer(packed_text)\n",
    "        \n",
    "        # 采用 pad_packed_sequence 把压紧的序列再填充回来\n",
    "        padded_hiddens, _ = pad_packed_sequence(lstm_hiddens, batch_first=True)\n",
    "\n",
    "        return padded_hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SelfAttention： 实质就是transformer 层自注意力机制\n",
    "\n",
    "class QKVAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention mechanism based on Query-Key-Value architecture. And\n",
    "    especially, when query == key == value, it's self-attention.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, query_dim, key_dim, value_dim, hidden_dim, output_dim, dropout_rate):\n",
    "        super(QKVAttention, self).__init__()\n",
    "\n",
    "        # Record hyper-parameters.\n",
    "        self.__query_dim = query_dim\n",
    "        self.__key_dim = key_dim\n",
    "        self.__value_dim = value_dim\n",
    "        self.__hidden_dim = hidden_dim\n",
    "        self.__output_dim = output_dim\n",
    "        self.__dropout_rate = dropout_rate\n",
    "\n",
    "        # Declare network structures.\n",
    "        self.__query_layer = nn.Linear(self.__query_dim, self.__hidden_dim)\n",
    "        self.__key_layer = nn.Linear(self.__key_dim, self.__hidden_dim)\n",
    "        self.__value_layer = nn.Linear(self.__value_dim, self.__output_dim)\n",
    "        self.__dropout_layer = nn.Dropout(p=self.__dropout_rate)\n",
    "\n",
    "    def forward(self, input_query, input_key, input_value):\n",
    "        \"\"\" The forward propagation of attention.\n",
    "\n",
    "        Here we require the first dimension of input key\n",
    "        and value are equal.\n",
    "\n",
    "        :param input_query: is query tensor, (n, d_q)\n",
    "        :param input_key:  is key tensor, (m, d_k)\n",
    "        :param input_value:  is value tensor, (m, d_v)\n",
    "        :return: attention based tensor, (n, d_h)\n",
    "        \"\"\"\n",
    "\n",
    "        # Linear transform to fine-tune dimension.\n",
    "        linear_query = self.__query_layer(input_query)   # (?, n, __hidden_dim)\n",
    "        linear_key = self.__key_layer(input_key)       # (?, m, __hidden_dim)\n",
    "        linear_value = self.__value_layer(input_value)   #  (?, m, __output_dim)\n",
    "\n",
    "        score_tensor = F.softmax(torch.matmul(\n",
    "            linear_query,\n",
    "            linear_key.transpose(-2, -1)    #  (?, __hidden_dim, m)\n",
    "        ), dim=-1) / math.sqrt(self.__hidden_dim)   # (?, n, m) * \n",
    "        forced_tensor = torch.matmul(score_tensor, linear_value)  # (?, n, m) * (?, m, __output_dim) : (?, n, __output_dim)\n",
    "        forced_tensor = self.__dropout_layer(forced_tensor)  \n",
    "        \n",
    "        #########################################################\n",
    "        # 思考题： 这里的实现，与原transformer实现有什么不一样？\n",
    "        #########################################################\n",
    "\n",
    "        return forced_tensor\n",
    "    \n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate):\n",
    "        super(SelfAttention, self).__init__()\n",
    "\n",
    "        # Record parameters.\n",
    "        self.__input_dim = input_dim\n",
    "        self.__hidden_dim = hidden_dim\n",
    "        self.__output_dim = output_dim\n",
    "        self.__dropout_rate = dropout_rate\n",
    "\n",
    "        # Record network parameters.\n",
    "        self.__dropout_layer = nn.Dropout(self.__dropout_rate)\n",
    "        self.__attention_layer = QKVAttention(\n",
    "            self.__input_dim, self.__input_dim, self.__input_dim,\n",
    "            self.__hidden_dim, self.__output_dim, self.__dropout_rate\n",
    "        )\n",
    "\n",
    "    def forward(self, input_x, seq_lens):\n",
    "        dropout_x = self.__dropout_layer(input_x)\n",
    "        attention_x = self.__attention_layer(\n",
    "            dropout_x, dropout_x, dropout_x\n",
    "        )\n",
    "\n",
    "        return attention_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于注意力机制的池化\n",
    "\n",
    "class UnflatSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    scores each element of the sequence with a linear layer \n",
    "    and uses the normalized scores to compute a context vector over the sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_hid, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.scorer = nn.Linear(d_hid, 1)      # d_hid 与下面的 d_feat 相同\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inp, lens):\n",
    "        batch_size, seq_len, d_feat = inp.size()\n",
    "        inp = self.dropout(inp)\n",
    "        \n",
    "        # 计算注意力score： [batch_size, seq_len]\n",
    "        scores = self.scorer(inp.contiguous().view(-1, d_feat)).view(batch_size, seq_len)\n",
    "        \n",
    "        # padding 的部分的 scores 置为大的负数，这样就相当于padding部分不会得到注意力\n",
    "        max_len = max(lens)\n",
    "        for i, l in enumerate(lens):\n",
    "            if l < max_len:\n",
    "                scores.data[i, l:] = -np.inf\n",
    "        scores = F.softmax(scores, dim=1)\n",
    "        \n",
    "        # 与 inp 相乘，求加权平均\n",
    "        context = scores.unsqueeze(2).expand_as(inp).mul(inp).sum(1)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "\n",
    "        self.__args = args\n",
    "\n",
    "        # Initialize an LSTM Encoder object.\n",
    "        self.__encoder = LSTMEncoder(\n",
    "            self.__args.word_embedding_dim,\n",
    "            self.__args.encoder_hidden_dim,\n",
    "            self.__args.dropout_rate\n",
    "        )\n",
    "\n",
    "        # Initialize an self-attention layer.\n",
    "        self.__attention = SelfAttention(\n",
    "            self.__args.word_embedding_dim,\n",
    "            self.__args.attention_hidden_dim,\n",
    "            self.__args.attention_output_dim,\n",
    "            self.__args.dropout_rate\n",
    "        )\n",
    "\n",
    "        self.__sentattention = UnflatSelfAttention(\n",
    "            self.__args.encoder_hidden_dim + self.__args.attention_output_dim,\n",
    "            self.__args.dropout_rate\n",
    "        )\n",
    "\n",
    "    def forward(self, word_tensor, seq_lens):\n",
    "        lstm_hiddens = self.__encoder(word_tensor, seq_lens)\n",
    "        attention_hiddens = self.__attention(word_tensor, seq_lens)\n",
    "        hiddens = torch.cat([attention_hiddens, lstm_hiddens], dim=2)\n",
    "        c = self.__sentattention(hiddens, seq_lens)\n",
    "        return hiddens, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder object for intent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 两个全连接层 中间加一个 LeakyReLU 激活函数\n",
    "\n",
    "__intent_decoder = nn.Sequential(\n",
    "    nn.Linear(self.__args.encoder_hidden_dim + self.__args.attention_output_dim,\n",
    "              self.__args.encoder_hidden_dim + self.__args.attention_output_dim),\n",
    "    nn.LeakyReLU(args.alpha),\n",
    "    nn.Linear(self.__args.encoder_hidden_dim + self.__args.attention_output_dim, self.__num_intent),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLOT decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAT网络结构\n",
    "\n",
    "<img src=\"./GAT.PNG\"  width=\"400\" height=\"400\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xavier_uniform_\n",
    "\n",
    "<img src=\"./xavier_uniform.webp\"  width=\"400\" height=\"400\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeakyReLU\n",
    "\n",
    "<img src=\"./leakyrelu.png\"  width=\"300\" height=\"400\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAT\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.zeros(size=(2 * out_features, 1)))\n",
    "        \n",
    "        # 均匀分布 ~ U(−X_0,X_O)\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        # adj：邻接矩阵\n",
    "        \n",
    "        h = torch.matmul(input, self.W)  # [B, N, out_features]\n",
    "        B, N = h.size()[0], h.size()[1]\n",
    "        \n",
    "        # [B, N * N, out_features] + [B, N * N, out_features] --> [B, N * N, 2 * out_features]\n",
    "        a_input = torch.cat([h.repeat(1, 1, N).view(B, N * N, -1), h.repeat(1, N, 1)], dim=2).view(\n",
    "            B, N, -1,        \n",
    "            2 * self.out_features\n",
    "        )   # --> [B, N, N, 2 * out_features]\n",
    "        # [B, N, N]\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(3))\n",
    "        \n",
    "        # 不连接的节点，则两两不注意;GAT一层只接收一阶邻域的消息传递\n",
    "        zero_vec = -9e15 * torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)   # 注意这里 torch.where(cond, A, B) 的用法\n",
    "        attention = F.softmax(attention, dim=2)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        \n",
    "        # [B, N, N] * [B, N, out_features] --》 [B, N, out_features]\n",
    "        h_prime = torch.matmul(attention, h)\n",
    "\n",
    "        if self.concat:           # 后续再看这个concat选项是何含义 \n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "    \n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, \n",
    "                 dropout, alpha, nheads, \n",
    "                 nlayers=2):\n",
    "        \"\"\"Dense version of GAT.\"\"\"\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.nlayers = nlayers\n",
    "        self.nheads = nheads\n",
    "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in\n",
    "                           range(nheads)]    # 注意这里concat=True\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "        if self.nlayers > 2:\n",
    "            for i in range(self.nlayers - 2):\n",
    "                for j in range(self.nheads):\n",
    "                    self.add_module('attention_{}_{}'.format(i + 1, j),\n",
    "                                    GraphAttentionLayer(nhid * nheads, nhid, dropout=dropout, alpha=alpha, concat=True))  # 注意这里concat=True\n",
    "\n",
    "        self.out_att = GraphAttentionLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False) # 注意这里concat=False\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        input = x\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=2)\n",
    "        \n",
    "        if self.nlayers > 2:\n",
    "            for i in range(self.nlayers - 2):\n",
    "                temp = []\n",
    "                x = F.dropout(x, self.dropout, training=self.training)\n",
    "                cur_input = x\n",
    "                for j in range(self.nheads):\n",
    "                    temp.append(self.__getattr__('attention_{}_{}'.format(i + 1, j))(x, adj))\n",
    "                x = torch.cat(temp, dim=2) + cur_input\n",
    "                \n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_att(x, adj))\n",
    "        return x + input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder structure based on unidirectional LSTM.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args, \n",
    "                 input_dim, \n",
    "                 hidden_dim, \n",
    "                 output_dim, \n",
    "                 dropout_rate, \n",
    "                 embedding_dim=None, \n",
    "                 extra_dim=None):      \n",
    "        \"\"\" Construction function for Decoder.\n",
    "\n",
    "        :param input_dim: input dimension of Decoder. In fact, it's encoder hidden size.\n",
    "        :param hidden_dim: hidden dimension of iterative LSTM.\n",
    "        :param output_dim: output dimension of Decoder. In fact, it's total number of slot.\n",
    "        :param dropout_rate: dropout rate of network which is only useful for embedding.\n",
    "        :param embedding_dim: if it's not None, the input and output are relevant.\n",
    "        :param extra_dim: if it's not None, the decoder receives information tensors.     # 没有用到，不需要理会\n",
    "        \"\"\" \n",
    "\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        self.__args = args\n",
    "        self.__input_dim = input_dim\n",
    "        self.__hidden_dim = hidden_dim\n",
    "        self.__output_dim = output_dim\n",
    "        self.__dropout_rate = dropout_rate\n",
    "        self.__embedding_dim = embedding_dim\n",
    "        self.__extra_dim = extra_dim\n",
    "\n",
    "        # If embedding_dim is not None, the output and input\n",
    "        # of this structure is relevant.\n",
    "        if self.__embedding_dim is not None:\n",
    "            self.__embedding_layer = nn.Embedding(output_dim, embedding_dim)\n",
    "            self.__init_tensor = nn.Parameter(\n",
    "                torch.randn(1, self.__embedding_dim),\n",
    "                requires_grad=True\n",
    "            )\n",
    "\n",
    "        # Make sure the input dimension of iterative LSTM.\n",
    "        if self.__extra_dim is not None and self.__embedding_dim is not None:\n",
    "            lstm_input_dim = self.__input_dim + self.__extra_dim + self.__embedding_dim\n",
    "        elif self.__extra_dim is not None:\n",
    "            lstm_input_dim = self.__input_dim + self.__extra_dim\n",
    "        elif self.__embedding_dim is not None:\n",
    "            lstm_input_dim = self.__input_dim + self.__embedding_dim\n",
    "        else:\n",
    "            lstm_input_dim = self.__input_dim\n",
    "\n",
    "        # Network parameter definition.\n",
    "        self.__dropout_layer = nn.Dropout(self.__dropout_rate)\n",
    "        self.__lstm_layer = nn.LSTM(\n",
    "            input_size=lstm_input_dim,\n",
    "            hidden_size=self.__hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "            dropout=self.__dropout_rate,\n",
    "            num_layers=1\n",
    "        )\n",
    "\n",
    "        self.__graph = GAT(\n",
    "            self.__hidden_dim,\n",
    "            self.__args.decoder_gat_hidden_dim,\n",
    "            self.__hidden_dim,\n",
    "            self.__args.gat_dropout_rate, \n",
    "            self.__args.alpha, \n",
    "            self.__args.n_heads,\n",
    "            self.__args.n_layers_decoder)\n",
    "\n",
    "        self.__linear_layer = nn.Linear(\n",
    "            self.__hidden_dim,\n",
    "            self.__output_dim\n",
    "        )\n",
    "\n",
    "    def forward(self, encoded_hiddens, seq_lens, forced_input=None,  # extra_input=None,\n",
    "                adj=None, intent_embedding=None):\n",
    "        \"\"\" Forward process for decoder.\n",
    "\n",
    "        :param encoded_hiddens: is encoded hidden tensors produced by encoder.\n",
    "        :param seq_lens: is a list containing lengths of sentence.\n",
    "        :param forced_input: is truth values of label, provided by teacher forcing.    # teacher forcing: 采用金标签来训练，而不是模型前面的输出结果；\n",
    "        :param extra_input: comes from another decoder as information tensor.\n",
    "        :return: is distribution of prediction labels.\n",
    "        \"\"\"\n",
    "\n",
    "        input_tensor = encoded_hiddens\n",
    "        output_tensor_list, sent_start_pos = [], 0\n",
    "        if self.__embedding_dim is not None and forced_input is not None:\n",
    "            \n",
    "            # forced_input： [B, seq_len]\n",
    "            forced_tensor = self.__embedding_layer(forced_input)[:, :-1]   # 为什么要去掉最后一个 ？\n",
    "            \n",
    "            # len(forced_tensor), 1, self.__embedding_dim; concat \n",
    "            prev_tensor = torch.cat((self.__init_tensor.unsqueeze(0).repeat(len(forced_tensor), 1, 1), forced_tensor),\n",
    "                                    dim=1)\n",
    "            combined_input = torch.cat([input_tensor, prev_tensor], dim=2)\n",
    "            dropout_input = self.__dropout_layer(combined_input)\n",
    "            packed_input = pack_padded_sequence(dropout_input, seq_lens, batch_first=True)\n",
    "            lstm_out, _ = self.__lstm_layer(packed_input)\n",
    "            lstm_out, _ = pad_packed_sequence(lstm_out, batch_first=True)\n",
    "            \n",
    "            for sent_i in range(0, len(seq_lens)):\n",
    "                # 一个句子一个句子的操作\n",
    "                if adj is not None:\n",
    "                    # [seq_lens[sent_i], 1, hidden_dim]  cat [seq_lens[sent_i], num_labels, hidden_dim]             \n",
    "                    lstm_out_i = torch.cat((lstm_out[sent_i][:seq_lens[sent_i]].unsqueeze(1),\n",
    "                                            intent_embedding.unsqueeze(0).repeat(seq_lens[sent_i], 1, 1)), dim=1)\n",
    "                    \n",
    "                    lstm_out_i = self.__graph(lstm_out_i, adj[sent_i].unsqueeze(0).repeat(seq_lens[sent_i], 1, 1))[:, 0]\n",
    "                else:\n",
    "                    lstm_out_i = lstm_out[sent_i][:seq_lens[sent_i]]\n",
    "                linear_out = self.__linear_layer(lstm_out_i)\n",
    "                output_tensor_list.append(linear_out)\n",
    "        else:\n",
    "            prev_tensor = self.__init_tensor.unsqueeze(0).repeat(len(seq_lens), 1, 1)\n",
    "            last_h, last_c = None, None\n",
    "            for word_i in range(seq_lens[0]):\n",
    "                combined_input = torch.cat((input_tensor[:, word_i].unsqueeze(1), prev_tensor), dim=2)\n",
    "                dropout_input = self.__dropout_layer(combined_input)\n",
    "                if last_h is None and last_c is None:\n",
    "                    lstm_out, (last_h, last_c) = self.__lstm_layer(dropout_input)\n",
    "                else:\n",
    "                    lstm_out, (last_h, last_c) = self.__lstm_layer(dropout_input, (last_h, last_c))\n",
    "\n",
    "                if adj is not None:\n",
    "                    lstm_out = torch.cat((lstm_out,\n",
    "                                          intent_embedding.unsqueeze(0).repeat(len(lstm_out), 1, 1)), dim=1)\n",
    "                    lstm_out = self.__graph(lstm_out, adj)[:, 0]\n",
    "\n",
    "                lstm_out = self.__linear_layer(lstm_out.squeeze(1))\n",
    "                output_tensor_list.append(lstm_out)\n",
    "\n",
    "                _, index = lstm_out.topk(1, dim=1)\n",
    "                # prev_tensor: 前面一步 预测 的 label 的 embedding\n",
    "                prev_tensor = self.__embedding_layer(index.squeeze(1)).unsqueeze(1)\n",
    "            output_tensor = torch.stack(output_tensor_list)\n",
    "            output_tensor_list = [output_tensor[:length, i] for i, length in enumerate(seq_lens)]\n",
    "\n",
    "        return torch.cat(output_tensor_list, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ModelManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型汇总类\n",
    "\n",
    "class ModelManager(nn.Module):\n",
    "\n",
    "    def __init__(self, args, num_word, num_slot, num_intent):\n",
    "        super(ModelManager, self).__init__()\n",
    "\n",
    "        self.__num_word = num_word\n",
    "        self.__num_slot = num_slot\n",
    "        self.__num_intent = num_intent\n",
    "        self.__args = args\n",
    "\n",
    "        # Initialize an embedding object.\n",
    "        self.__embedding = nn.Embedding(\n",
    "            self.__num_word,\n",
    "            self.__args.word_embedding_dim\n",
    "        )\n",
    "\n",
    "        self.G_encoder = Encoder(args)\n",
    "        # Initialize an Decoder object for intent.\n",
    "        self.__intent_decoder = nn.Sequential(\n",
    "            nn.Linear(self.__args.encoder_hidden_dim + self.__args.attention_output_dim,\n",
    "                      self.__args.encoder_hidden_dim + self.__args.attention_output_dim),\n",
    "            nn.LeakyReLU(args.alpha),\n",
    "            nn.Linear(self.__args.encoder_hidden_dim + self.__args.attention_output_dim, self.__num_intent),\n",
    "        )\n",
    "\n",
    "        self.__intent_embedding = nn.Parameter(\n",
    "            torch.FloatTensor(self.__num_intent, self.__args.intent_embedding_dim))  # 191, 32\n",
    "        nn.init.normal_(self.__intent_embedding.data)\n",
    "\n",
    "        # Initialize an Decoder object for slot.\n",
    "        self.__slot_decoder = LSTMDecoder(\n",
    "            args,\n",
    "            self.__args.encoder_hidden_dim + self.__args.attention_output_dim,\n",
    "            self.__args.slot_decoder_hidden_dim,\n",
    "            self.__num_slot, \n",
    "            self.__args.dropout_rate,\n",
    "            embedding_dim=self.__args.slot_embedding_dim)\n",
    "\n",
    "    def show_summary(self):\n",
    "        \"\"\"\n",
    "        print the abstract of the defined model.\n",
    "        \"\"\"\n",
    "\n",
    "        print('Model parameters are listed as follows:\\n')\n",
    "\n",
    "        print('\\tnumber of word:                            {};'.format(self.__num_word))\n",
    "        print('\\tnumber of slot:                            {};'.format(self.__num_slot))\n",
    "        print('\\tnumber of intent:\t\t\t\t\t\t    {};'.format(self.__num_intent))\n",
    "        print('\\tword embedding dimension:\t\t\t\t    {};'.format(self.__args.word_embedding_dim))\n",
    "        print('\\tencoder hidden dimension:\t\t\t\t    {};'.format(self.__args.encoder_hidden_dim))\n",
    "        print('\\tdimension of intent embedding:\t\t    \t{};'.format(self.__args.intent_embedding_dim))\n",
    "        print('\\tdimension of slot embedding:\t\t\t    {};'.format(self.__args.slot_embedding_dim))\n",
    "        print('\\tdimension of slot decoder hidden:  \t    {};'.format(self.__args.slot_decoder_hidden_dim))\n",
    "        print('\\thidden dimension of self-attention:        {};'.format(self.__args.attention_hidden_dim))\n",
    "        print('\\toutput dimension of self-attention:        {};'.format(self.__args.attention_output_dim))\n",
    "\n",
    "        print('\\nEnd of parameters show. Now training begins.\\n\\n')\n",
    "\n",
    "    def generate_adj_gat(self, index, batch):\n",
    "        # slot node: 作为编号0\n",
    "        \n",
    "        intent_idx_ = [[torch.tensor(0)] for i in range(batch)]\n",
    "        for item in index:\n",
    "            intent_idx_[item[0]].append(item[1] + 1)    # slot node 与 预测的意图node相连\n",
    "        intent_idx = intent_idx_\n",
    "        \n",
    "        # [B, N, N]\n",
    "        adj = torch.cat([torch.eye(self.__num_intent + 1).unsqueeze(0) for i in range(batch)])\n",
    "        for i in range(batch):\n",
    "            for j in intent_idx[i]:\n",
    "                adj[i, j, intent_idx[i]] = 1.   # 预测到的意图标签两两相连\n",
    "        if self.__args.row_normalized:\n",
    "            adj = normalize_adj(adj)\n",
    "        if self.__args.gpu:\n",
    "            adj = adj.cuda()\n",
    "        return adj\n",
    "\n",
    "    def forward(self, text, seq_lens, n_predicts=None, forced_slot=None, forced_intent=None):\n",
    "        word_tensor = self.__embedding(text)\n",
    "        g_hiddens, g_c = self.G_encoder(word_tensor, seq_lens)\n",
    "        pred_intent = self.__intent_decoder(g_c)\n",
    "        \n",
    "        # 根据 threshold 来决定是否预测该标签\n",
    "        # .nonzero(): 输出非零元素的坐标；[(i, j ), ()]\n",
    "        intent_index = (torch.sigmoid(pred_intent) > self.__args.threshold).nonzero()\n",
    "        adj = self.generate_adj_gat(intent_index, len(pred_intent))\n",
    "        \n",
    "        pred_slot = self.__slot_decoder(\n",
    "            g_hiddens, seq_lens,\n",
    "            forced_input=forced_slot,\n",
    "            adj=adj,\n",
    "            intent_embedding=self.__intent_embedding\n",
    "        )\n",
    "        \n",
    "        # 两种不同的模式： n_predicts=None,一种针对训练，\n",
    "        if n_predicts is None:\n",
    "            return F.log_softmax(pred_slot, dim=1), pred_intent\n",
    "        else:\n",
    "            _, slot_index = pred_slot.topk(n_predicts, dim=1)\n",
    "            intent_index = (torch.sigmoid(pred_intent) > self.__args.threshold).nonzero()\n",
    "\n",
    "            return slot_index.cpu().data.numpy().tolist(), intent_index.cpu().data.numpy().tolist()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
