{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型构建与损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**目录：**\n",
    "1. JointBERT模型\n",
    "    - 分类层;\n",
    "    - CRF层;\n",
    "\n",
    "\n",
    "2. 损失函数计算\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import TensorDataset, RandomSampler, DataLoader\n",
    "\n",
    "# 以BERT为预训练模型进行讲解\n",
    "from transformers import BertPreTrainedModel, BertModel, BertConfig\n",
    "from torchcrf import CRF  # pip install pytorch-crf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 两个分类任务各自的MLP层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intent分类的MLP全连接层\n",
    "class IntentClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_intent_labels, dropout_rate=0.):\n",
    "        super(IntentClassifier, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(input_dim, num_intent_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, input_dim]\n",
    "        x = self.dropout(x)\n",
    "        return self.linear(x)\n",
    "\n",
    "    \n",
    "# slot分类的MLP全连接层\n",
    "class SlotClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_slot_labels, dropout_rate=0.):\n",
    "        super(SlotClassifier, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(input_dim, num_slot_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, max_seq_len, input_dim]\n",
    "        x = self.dropout(x)\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主要的模型框架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class JointBERT(BertPreTrainedModel):\n",
    "class JointBERT(nn.Module):\n",
    "    def __init__(self, config, args, intent_label_lst, slot_label_lst):\n",
    "        super(JointBERT, self).__init__(config)\n",
    "        self.args = args\n",
    "        self.num_intent_labels = len(intent_label_lst)\n",
    "        self.num_slot_labels = len(slot_label_lst)\n",
    "        \n",
    "        self.bert = BertModel(config=config)  # Load pretrained bert\n",
    "\n",
    "        self.intent_classifier = IntentClassifier(config.hidden_size, self.num_intent_labels, args.dropout_rate)\n",
    "        self.slot_classifier = SlotClassifier(config.hidden_size, self.num_slot_labels, args.dropout_rate)\n",
    "\n",
    "        if args.use_crf:\n",
    "            self.crf = CRF(num_tags=self.num_slot_labels, batch_first=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, intent_label_ids, slot_labels_ids):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids)  # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "        sequence_output = outputs[0]  # [bsz, seq_len, hidden_dim]\n",
    "        pooled_output = outputs[1]  # [CLS]上的输出, BertPooler module, MLP, tanh, \n",
    "\n",
    "        intent_logits = self.intent_classifier(pooled_output)\n",
    "        slot_logits = self.slot_classifier(sequence_output)\n",
    "\n",
    "        total_loss = 0\n",
    "        # 1. 计算intent分类任务的loss\n",
    "        if intent_label_ids is not None:\n",
    "            if self.num_intent_labels == 1:   # STS-B： 回归任务\n",
    "                intent_loss_fct = nn.MSELoss()\n",
    "                intent_loss = intent_loss_fct(intent_logits.view(-1), intent_label_ids.view(-1))\n",
    "            else:\n",
    "                intent_loss_fct = nn.CrossEntropyLoss()\n",
    "                intent_loss = intent_loss_fct(\n",
    "                    intent_logits.view(-1, self.num_intent_labels), \n",
    "                    intent_label_ids.view(-1)\n",
    "                )\n",
    "            total_loss += intent_loss\n",
    "\n",
    "        # 2. Slot Softmax\n",
    "        if slot_labels_ids is not None:\n",
    "            if self.args.use_crf:\n",
    "                slot_loss = self.crf(\n",
    "                    slot_logits, \n",
    "                    slot_labels_ids, \n",
    "                    mask=attention_mask.byte(), \n",
    "                    reduction='mean',\n",
    "                )\n",
    "                slot_loss = -1 * slot_loss  # negative log-likelihood\n",
    "            else:\n",
    "                # 指定ignore_index\n",
    "                slot_loss_fct = nn.CrossEntropyLoss(ignore_index=self.args.ignore_index)\n",
    "                # Only keep active parts of the loss\n",
    "                # 只计算非padding部分的loss\n",
    "                if attention_mask is not None:\n",
    "                    active_loss = attention_mask.view(-1) == 1   # [B * L, 1]\n",
    "                    print(\"active_loss: \", active_loss)\n",
    "                    \n",
    "                    active_logits = slot_logits.view(\n",
    "                        -1, self.num_slot_labels\n",
    "                    )[active_loss]  # [B * L , num_slot_labels]\n",
    "                    print(\"active_logits: \", active_logits)\n",
    "                    \n",
    "                    active_labels = slot_labels_ids.view(-1)[active_loss]   # [-1, 1]\n",
    "                    print(\"active_labels: \", active_labels)\n",
    "                    \n",
    "                    slot_loss = slot_loss_fct(active_logits, active_labels)\n",
    "                    \n",
    "                else:\n",
    "                    slot_loss = slot_loss_fct(\n",
    "                        slot_logits.view(-1, self.num_slot_labels), \n",
    "                        slot_labels_ids.view(-1)\n",
    "                    )\n",
    "            # total loss = intent_loss + coef*slot_loss\n",
    "            total_loss += self.args.slot_loss_coef * slot_loss\n",
    "\n",
    "        outputs = ((intent_logits, slot_logits),) + outputs[2:]  # add hidden states and attention if they are here\n",
    "\n",
    "        outputs = (total_loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions) # Logits is a tuple of intent and slot logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失函数 CrossEntropyLoss\n",
    "Pytorch中CrossEntropyLoss()函数的主要是将softmax -> log -> NLLLoss合并到一块得到的结果。\n",
    "$$L=- \\sum_{i=1}^{N}y_i* \\log \\hat{y_i}$$\n",
    "$y_i$是真正类别的one-hot分布，只有真实类别的概率为1，其他都是0，$\\hat{y_i}$是经由softmax后的分布\n",
    "\n",
    "- softmax将输出数据规范化为一个概率分布。\n",
    "\n",
    "- 然后将Softmax之后的结果取log\n",
    "\n",
    "- 输入负对数损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 举例查看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, DistilBertConfig, AlbertConfig\n",
    "from transformers import BertTokenizer, DistilBertTokenizer, AlbertTokenizer\n",
    "\n",
    "from JointBERT.model import JointBERT, JointDistilBERT, JointAlbert\n",
    "from JointBERT.utils import init_logger, load_tokenizer, get_intent_labels, get_slot_labels\n",
    "from JointBERT.data_loader import load_and_cache_examples\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'bert': (BertConfig, JointBERT, BertTokenizer),\n",
    "    'distilbert': (DistilBertConfig, JointDistilBERT, DistilBertTokenizer),\n",
    "    'albert': (AlbertConfig, JointAlbert, AlbertTokenizer)\n",
    "}\n",
    "\n",
    "MODEL_PATH_MAP = {\n",
    "    'bert': 'resources/bert_base_uncased',\n",
    "    'distilbert': 'distilbert-base-uncased',\n",
    "    'albert': 'albert-xxlarge-v1',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先构建参数\n",
    "class Args():\n",
    "    task =  None\n",
    "    data_dir =  None\n",
    "    intent_label_file =  None\n",
    "    slot_label_file =  None\n",
    "\n",
    "args = Args()\n",
    "args.task = \"atis\"\n",
    "args.data_dir = \"./data\"\n",
    "args.intent_label_file = \"intent_label.txt\"\n",
    "args.slot_label_file = \"slot_label.txt\"\n",
    "args.max_seq_len = 50\n",
    "args.model_type = \"bert\"\n",
    "args.model_dir = \"experiments/jointbert_0\"\n",
    "args.model_name_or_path = MODEL_PATH_MAP[args.model_type]\n",
    "\n",
    "args.ignore_index = -100\n",
    "\n",
    "args.train_batch_size = 4\n",
    "\n",
    "args.dropout_rate = 0.1\n",
    "args.use_crf = False\n",
    "\n",
    "args.slot_loss_coef = 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load_tokenizer(args)\n",
    "\n",
    "\n",
    "config = MODEL_CLASSES[args.model_type][0].from_pretrained(args.model_name_or_path)\n",
    "\n",
    "intent_label_lst = get_intent_labels(args)\n",
    "slot_label_lst = get_slot_labels(args)\n",
    "\n",
    "num_intent_labels = len(intent_label_lst)\n",
    "num_slot_labels = len(slot_label_lst)\n",
    "\n",
    "model = JointBERT(config, args, intent_label_lst, slot_label_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:  torch.Size([4, 50])\n",
      "slot_labels_ids:  torch.Size([4, 50])\n",
      "slot_labels_ids:  tensor([[-100,    2,    2,    2,   63,    2,    2,   73,    2,  114, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100],\n",
      "        [-100,   48,   48,    2,    2,   73,    2,  114, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100],\n",
      "        [-100,    2,    2,    2,   81,    2,    2,   73,    2,  114,  115,    2,\n",
      "           38,   41,   39, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100],\n",
      "        [-100,    2,    2,    2,    2,    2,    2,    2,    4,   65,    2,   73,\n",
      "            2,  114,    2,    2,    4,   65,    2,   73,    2,  114, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100]])\n",
      "sequence_output:  torch.Size([4, 50, 768])\n",
      "pooled_output:  torch.Size([4, 768])\n",
      "intent_logits:  torch.Size([4, 22])\n",
      "slot_logits:  torch.Size([4, 50, 122])\n",
      "active_loss:  torch.Size([200])\n",
      "slot_logits:  torch.Size([4, 50, 122])\n",
      "active_logits:  torch.Size([59, 122])\n",
      "active_labels:  torch.Size([59])\n",
      "slot_loss:  tensor(5.9268, grad_fn=<NllLossBackward>)\n",
      "slot_loss:  tensor(5.9268, grad_fn=<NllLossBackward>)\n",
      "input_ids:  torch.Size([4, 50])\n",
      "slot_labels_ids:  torch.Size([4, 50])\n",
      "slot_labels_ids:  tensor([[-100,    2,    2,    2,    2,    2,  112,  113,  113, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100],\n",
      "        [-100,    2,    2,    2,    2,    2,    5,    6,    2,   73,    2,  114,\n",
      "          115, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100],\n",
      "        [-100,    2,    2,    2,    2,    2,  120,    2,    2,    8,    9, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100],\n",
      "        [-100,    2,    2,    2,    2,    2,    2,   32,    2,    2,    2,    2,\n",
      "           58, -100,   59,    2,   73,   74,    2,  114,    2,    2,    2,   38,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100]])\n",
      "sequence_output:  torch.Size([4, 50, 768])\n",
      "pooled_output:  torch.Size([4, 768])\n",
      "intent_logits:  torch.Size([4, 22])\n",
      "slot_logits:  torch.Size([4, 50, 122])\n",
      "active_loss:  torch.Size([200])\n",
      "slot_logits:  torch.Size([4, 50, 122])\n",
      "active_logits:  torch.Size([61, 122])\n",
      "active_labels:  torch.Size([61])\n",
      "slot_loss:  tensor(6.1044, grad_fn=<NllLossBackward>)\n",
      "slot_loss:  tensor(6.1044, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# load dataset \n",
    "train_dataset = load_and_cache_examples(args, tokenizer, mode=\"train\")\n",
    "\n",
    "# torch自带的sampler类，功能是每次返回一个随机的样本索引\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "# 使用dataloader输出batch\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
    "\n",
    "device = \"cpu\"\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    batch = tuple(t.to(device) for t in batch) # 将batch上传到显卡\n",
    "    inputs = {\"input_ids\": batch[0],\n",
    "              \"attention_mask\": batch[1],\n",
    "              \"token_type_ids\": batch[2],\n",
    "              \"intent_label_ids\": batch[3],\n",
    "              \"slot_labels_ids\": batch[4]}\n",
    "    \n",
    "    input_ids = inputs[\"input_ids\"]  # [B, L]    \n",
    "    \n",
    "    attention_mask = inputs[\"attention_mask\"]  # [B, L]\n",
    "    token_type_ids = inputs[\"token_type_ids\"]  # [B, L]\n",
    "    intent_label_ids = inputs[\"intent_label_ids\"]   # [B, ]\n",
    "    \n",
    "    slot_labels_ids = inputs[\"slot_labels_ids\"]   # [B, L]\n",
    "    \n",
    "    \n",
    "    if step > 1:\n",
    "        break\n",
    "        \n",
    "    print(\"input_ids: \", input_ids.shape)\n",
    "    print(\"slot_labels_ids: \", slot_labels_ids.shape)\n",
    "    print(\"slot_labels_ids: \", slot_labels_ids)\n",
    "    \n",
    "    outputs = model.bert(input_ids, attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids)  # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "    \n",
    "    \n",
    "    sequence_output = outputs[0]   # [B, L, H]\n",
    "    print(\"sequence_output: \", sequence_output.shape)\n",
    "    \n",
    "    pooled_output = outputs[1]   # [B, H]\n",
    "    print(\"pooled_output: \", pooled_output.shape)\n",
    "    \n",
    "    # 计算intent分类的损失\n",
    "    intent_logits = model.intent_classifier(pooled_output)   # [B, 22]\n",
    "    print(\"intent_logits: \", intent_logits.shape)\n",
    "    \n",
    "    intent_loss_fct = nn.CrossEntropyLoss()\n",
    "    intent_loss = intent_loss_fct(intent_logits.view(-1, num_intent_labels), intent_label_ids.view(-1))\n",
    "        \n",
    "    ####################################################################################\n",
    "    # 采用JointBERT模型的写法，计算 active loss，也就是只计算句子中的非padding部分的损失\n",
    "    ####################################################################################\n",
    "    \n",
    "    # [CLS], [SEP], \n",
    "    # word 非开始tokens， \n",
    "    \n",
    "    slot_logits = model.slot_classifier(sequence_output)\n",
    "    print(\"slot_logits: \", slot_logits.shape)\n",
    "    \n",
    "    \n",
    "    active_loss = attention_mask.view(-1) == 1\n",
    "    print(\"active_loss: \", active_loss.shape)\n",
    "    \n",
    "    active_logits = slot_logits.view(-1, num_slot_labels)[active_loss]\n",
    "    print(\"slot_logits: \", slot_logits.shape)\n",
    "    print(\"active_logits: \", active_logits.shape)\n",
    "\n",
    "    active_labels = slot_labels_ids.view(-1)[active_loss]\n",
    "    print(\"active_labels: \", active_labels.shape)\n",
    "    \n",
    "    slot_loss_fct = nn.CrossEntropyLoss()\n",
    "    slot_loss = slot_loss_fct(active_logits, active_labels)\n",
    "    print(\"slot_loss: \", slot_loss)\n",
    "    \n",
    "    ####################################################################################\n",
    "    # 直接计算: 利用 ignore_index\n",
    "    ####################################################################################\n",
    "    slot_loss_fct = nn.CrossEntropyLoss(ignore_index=args.ignore_index)\n",
    "    slot_loss = slot_loss_fct(\n",
    "        slot_logits.view(-1, num_slot_labels), \n",
    "        slot_labels_ids.view(-1)\n",
    "    )\n",
    "    print(\"slot_loss: \", slot_loss)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slot_loss:  tensor(66.9170, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#  改为使用crf : use_crf =True\n",
    "\n",
    "args.use_crf = True\n",
    "model = JointBERT(config, args, intent_label_lst, slot_label_lst)\n",
    "\n",
    "# load dataset \n",
    "train_dataset = load_and_cache_examples(args, tokenizer, mode=\"train\")\n",
    "\n",
    "# torch自带的sampler类，功能是每次返回一个随机的样本索引\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "# 使用dataloader输出batch\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
    "\n",
    "device = \"cpu\"\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    batch = tuple(t.to(device) for t in batch) # 将batch上传到显卡\n",
    "    inputs = {\"input_ids\": batch[0],\n",
    "              \"attention_mask\": batch[1],\n",
    "              \"token_type_ids\": batch[2],\n",
    "              \"intent_label_ids\": batch[3],\n",
    "              \"slot_labels_ids\": batch[4]}\n",
    "    \n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    token_type_ids = inputs[\"token_type_ids\"]\n",
    "    intent_label_ids = inputs[\"intent_label_ids\"]\n",
    "    slot_labels_ids = inputs[\"slot_labels_ids\"]\n",
    "    \n",
    "    if step > 0:\n",
    "        break\n",
    "    \n",
    "    outputs = model.bert(input_ids, attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids)  # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "    sequence_output = outputs[0]\n",
    "    \n",
    "    slot_logits = model.slot_classifier(sequence_output)\n",
    "    \n",
    "    slot_loss = model.crf(slot_logits, slot_labels_ids, mask=attention_mask.byte(), reduction='mean')\n",
    "    slot_loss = -1 * slot_loss  # negative log-likelihood\n",
    "    print(\"slot_loss: \", slot_loss)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
